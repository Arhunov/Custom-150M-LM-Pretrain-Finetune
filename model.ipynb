{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08a3874d-e3e2-4344-bccc-5be05a89be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch._dynamo as dynamo\n",
    "import math\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoTokenizer, TrainerCallback, pipeline\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import zlib\n",
    "from typing import List, Dict\n",
    "import wandb\n",
    "import time\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92ada6c5-3f1c-4b66-ace7-1ac63a5068b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x79af5954d2d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, device=0)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "#os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = '1'\n",
    "#torch._inductor.config.triton.unique_kernel_names = True\n",
    "#torch._inductor.config.triton.cudagraphs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7fddcc41-0329-4782-8f04-86d1a3f1b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embeddings (RoPE) module.\n",
    "    Injects position info via complex rotations directly into attention queries/keys.\n",
    "    Unlike classic sinusoidal embeddings that add position vectors,\n",
    "    RoPE applies rotations preserving relative position relationships,\n",
    "    enabling better extrapolation to longer sequences beyond training.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 4096, base: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Compute inverse frequencies for each pair of dimensions\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)) \n",
    "        # Positions from 0 to max_seq_len\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32) \n",
    "        # Outer product gives [pos, dim/2] matrix of phase angles\n",
    "        freqs = torch.outer(t, inv_freq) \n",
    "        # Convert phases to complex rotations: cos(θ) + i·sin(θ)\n",
    "        self.register_buffer(\"freqs_cis\", torch.polar(torch.ones_like(freqs), freqs)) \n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0):\n",
    "        # x: [batch, seq_len, num_heads, head_dim]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # Convert last dim into complex numbers: [..., d] → [..., d/2] as complex\n",
    "        x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "        \n",
    "        # Select matching rotary angles for current sequence length and start_pos\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seq_len] # [seq_len, d/2]\n",
    "        freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2) # [1, seq_len, 1, d/2]\n",
    "\n",
    "        # Apply rotation: complex multiplication performs the position encoding\n",
    "        x_out = torch.view_as_real(x_ * freqs_cis).flatten(3)\n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be147339-9f6c-4328-beb0-91df399baaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU feed-forward module.\n",
    "    Uses gated linear units with SiLU activation, which improves over ReLU by:\n",
    "    - Providing smooth, non-zero gradients everywhere for better training stability\n",
    "    - Enabling multiplicative gating that enhances model capacity and expressiveness\n",
    "    This leads to faster convergence and better generalization compared to classic ReLU-based FFNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int = 256, bias: bool = False):\n",
    "        super().__init__()\n",
    "        # Adjust hidden_dim to be multiple of `multiple_of` for hardware efficiency\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        # Linear layers: input→hidden, input→hidden (gate), hidden→output\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=bias)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=bias)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply gated activation: w2( SiLU(w1(x)) * w3(x) )\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75a497c4-e924-49b1-9a14-bf461da3040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention (GQA) module with Rotary Positional Embeddings (RoPE) and KV caching.\n",
    "    Uses multiple query heads grouped to share a smaller number of key and value heads.\n",
    "    This reduces memory and compute cost while maintaining better quality than Multi-Query Attention (MQA).\n",
    "    Each group of query heads attends to the same key/value head, enabling faster inference with improved flexibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_head: int, num_kv_heads: int, rope: RotaryPositionalEmbeddings | None, bias: bool = False):\n",
    "        super().__init__()\n",
    "        assert n_head % num_kv_heads == 0, \"n_head must be divisible by num_kv_heads\"\n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        self.n_head = n_head\n",
    "        self.kv_head = num_kv_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.rope = rope\n",
    "\n",
    "        # Query projection: one per head (shape: [d_model, n_head*head_dim])\n",
    "        self.wq = nn.Linear(d_model, n_head * self.head_dim, bias=bias)\n",
    "        \n",
    "        # Single key and value projections shared across heads (shape: [d_model, head_dim])\n",
    "        self.wk = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=bias)\n",
    "        self.wv = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=bias)\n",
    "\n",
    "        # Output projection to restore original dimension\n",
    "        self.wo = nn.Linear(n_head * self.head_dim, d_model, bias=bias)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x_q: torch.Tensor, \n",
    "        x_kv: torch.Tensor | None = None, \n",
    "        mask: torch.Tensor | None = None, \n",
    "        kv_cache: tuple | None = None, \n",
    "        is_causal: bool = False\n",
    "    ):\n",
    "        # In self-attention, K and V are derived from the same input as Q. \n",
    "        is_self_attention = x_kv is None\n",
    "        if is_self_attention:\n",
    "            x_kv = x_q\n",
    "\n",
    "        batch_size, seq_len_q, _ = x_q.shape\n",
    "        _, seq_len_kv, _ = x_kv.shape\n",
    "        \n",
    "        # Compute Q, K, V projections\n",
    "        xq, xk, xv = self.wq(x_q), self.wk(x_kv), self.wv(x_kv)\n",
    "        \n",
    "        # Reshape Q: [batch, seq_len, n_head * head_dim] → [batch, seq_len, n_head, head_dim]\n",
    "        xq = xq.view(batch_size, seq_len_q, self.n_head, self.head_dim)\n",
    "        # Reshape K, V: [batch, seq_len, head_dim] → [batch, seq_len, num_kv_heads, head_dim]\n",
    "        xk = xk.view(batch_size, seq_len_kv, self.kv_head, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len_kv, self.kv_head, self.head_dim)\n",
    "        \n",
    "        # Apply Rotary Positional Embeddings to Q and K only if rope handed\n",
    "        if self.rope is not None:\n",
    "            start_pos = 0\n",
    "            if kv_cache is not None and kv_cache[0] is not None:\n",
    "                start_pos = kv_cache[0].shape[1]\n",
    "            xq = self.rope(xq, start_pos)\n",
    "            xk = self.rope(xk, start_pos)\n",
    "            \n",
    "\n",
    "        # Key-Value caching for efficient autoregressive decoding\n",
    "        if is_self_attention and kv_cache is not None and kv_cache[0] is not None:\n",
    "            cached_k, cached_v = kv_cache\n",
    "            # Append new keys and values to cached tensors along sequence dimension\n",
    "            xk = torch.cat([cached_k, xk], dim=1)\n",
    "            xv = torch.cat([cached_v, xv], dim=1)\n",
    "        \n",
    "        # Update cache to return\n",
    "        updated_kv_cache = (xk.clone().detach(), xv.clone().detach())\n",
    "\n",
    "        # enable_gqa=True in scaled_dot_product_attention will do this:\n",
    "        # Repeat K and V across all heads to match Q's head dimension\n",
    "        # Shape: [batch, seq_len, num_kv_heads, head_dim] → [batch, seq_len, n_head, head_dim]\n",
    "        #n_repeats = self.n_head // self.kv_head\n",
    "        keys=xk#keys = xk.repeat_interleave(n_repeats, dim=2)\n",
    "        values=xv#values = xv.repeat_interleave(n_repeats, dim=2)\n",
    "\n",
    "        # Prepare tensors for scaled dot-product attention\n",
    "        # Transpose to shape [batch, n_head, seq_len, head_dim]\n",
    "        xq, keys, values = xq.transpose(1, 2), keys.transpose(1, 2), values.transpose(1, 2)\n",
    "\n",
    "        # Perform attention with optional causal masking\n",
    "        if is_causal:\n",
    "            device = xq.device\n",
    "            full_seq_len_kv = xk.shape[1]\n",
    "            causal_mask = torch.tril(torch.ones((1, 1, seq_len_q, full_seq_len_kv), device=device, dtype=torch.bool), diagonal=0)\n",
    "            if mask is not None:\n",
    "                mask = mask & causal_mask\n",
    "            else:\n",
    "                mask = causal_mask\n",
    "\n",
    "        if seq_len_q == 1:\n",
    "            mask = None\n",
    "        output = F.scaled_dot_product_attention(xq, keys, values, attn_mask=mask, enable_gqa=True)\n",
    "        \n",
    "        # Reshape back to [batch, seq_len, n_head * head_dim]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)\n",
    "\n",
    "        # Final linear projection to output dimension\n",
    "        return self.wo(output), updated_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0edadd9a-e197-4bb5-857b-930e797ec679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern Transformer Encoder Layer employing:\n",
    "    - Pre-LayerNorm for better training stability and gradient flow,\n",
    "      replacing the original post-LN design.\n",
    "    - Dropout strategically placed after attention and feed-forward blocks\n",
    "      to prevent overfitting.\n",
    "      \"\"\"\n",
    "    def __init__(self, d_model: int, n_head: int, num_kv_heads: int, rope: RotaryPositionalEmbeddings, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = GroupedQueryAttention(d_model, n_head, num_kv_heads, rope)\n",
    "        self.feed_forward = SwiGLU(dim=d_model, hidden_dim=None)\n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.ffn_dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None):\n",
    "        # Pre-LN\n",
    "        x = self.attention_norm(x)\n",
    "        h, _ = self.self_attn(x_q=x, mask=mask)\n",
    "        x = x + self.attn_dropout(h)\n",
    "        out = x + self.ffn_dropout(self.feed_forward(self.ffn_norm(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a188469a-7724-4ad4-81f9-a3211fc27433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern Transformer Decoder Layer combining:\n",
    "    - Pre-LayerNorm applied before each sub-layer for stable and efficient training.\n",
    "    - Residual connections and dropout after each sub-layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_head: int, num_kv_heads: int, rope: RotaryPositionalEmbeddings, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        #self.self_attn = nn.MultiheadAttention(d_model, n_head, batch_first=True, bias=False)\n",
    "        #self.cross_attn = nn.MultiheadAttention(d_model, n_head, batch_first=True, bias=False)\n",
    "        self.self_attn = GroupedQueryAttention(d_model, n_head, num_kv_heads, rope=rope)\n",
    "        self.cross_attn = GroupedQueryAttention(d_model, n_head, num_kv_heads, rope=None)\n",
    "        self.feed_forward = SwiGLU(dim=d_model, hidden_dim=None)\n",
    "        \n",
    "        self.sa_norm = nn.LayerNorm(d_model)\n",
    "        self.ca_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.sa_dropout = nn.Dropout(dropout)\n",
    "        self.ca_dropout = nn.Dropout(dropout)\n",
    "        self.ffn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        memory: torch.Tensor,\n",
    "        src_mask: torch.Tensor | None,\n",
    "        tgt_mask: torch.Tensor | None,\n",
    "        self_attn_kv_cache: tuple | None = None,\n",
    "    ):\n",
    "        # Self-Attention: Pre-LN, causal mask, KV cache, residual + dropout\n",
    "        x_sa = self.sa_norm(x)\n",
    "        h, updated_sa_kv = self.self_attn(x_q=x_sa, mask=tgt_mask, kv_cache=self_attn_kv_cache, is_causal=True)\n",
    "        x = x + self.sa_dropout(h)\n",
    "        \n",
    "        # Cross-Attention: Pre-LN, attends encoder output with padding mask, residual + dropout\n",
    "        if self.cross_attn is not None:\n",
    "            x_ca = self.ca_norm(x)\n",
    "            h, _ = self.cross_attn(x_q=x_ca, x_kv=memory, mask=src_mask)\n",
    "            x = x + self.ca_dropout(h)\n",
    "        \n",
    "        # Feed-Forward: Pre-LN, SwiGLU, residual + dropout\n",
    "        out = x + self.ffn_dropout(self.feed_forward(self.ffn_norm(x)))\n",
    "        \n",
    "        return out, updated_sa_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "728d1740-2e06-4dc8-a32c-be01fe6c13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_head: int, num_kv_heads: int, num_encoder_layers: int, num_decoder_layers: int, dropout: float = 0.1, max_seq_len: int = 1024, label_smothing=0.08):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.config = lambda: None\n",
    "        self.config.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        rope = RotaryPositionalEmbeddings(d_model // n_head, max_seq_len=max_seq_len)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            ModernEncoderLayer(d_model, n_head, num_kv_heads, rope, dropout) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            ModernDecoderLayer(d_model, n_head, num_kv_heads, rope, dropout) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.encoder_norm = nn.LayerNorm(d_model, eps=5e-5)\n",
    "        self.decoder_norm = nn.LayerNorm(d_model, eps=5e-5)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.apply(self._apply_eps)\n",
    "\n",
    "        self.label_smoothing=label_smothing\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, labels=None):\n",
    "        # Encoder\n",
    "        h = self.embedding(input_ids) \n",
    "        h = self.embedding_dropout(h)\n",
    "        \n",
    "        # Padding mask\n",
    "        # [batch_size, seq_len]\n",
    "        src_mask = (attention_mask == 1)#.unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (decoder_attention_mask == 1)#.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # [B, S] -> [B, 1, S] | [B, S, 1] -> [B, S, S]\n",
    "        src_mask = src_mask[:, None, :] | src_mask[:, :, None]\n",
    "        tgt_mask = tgt_mask[:, None, :] | tgt_mask[:, :, None]\n",
    "\n",
    "        # [B, S, S] -> [B, 1, S, S]\n",
    "        src_mask = src_mask.unsqueeze(1)\n",
    "        tgt_mask = tgt_mask.unsqueeze(1)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h, src_mask)\n",
    "        h = self.encoder_norm(h)\n",
    "        memory = h\n",
    "\n",
    "        # Decoder\n",
    "        h = self.embedding(decoder_input_ids)\n",
    "        h = self.embedding_dropout(h)\n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h, memory, src_mask=src_mask, tgt_mask=tgt_mask)[0]\n",
    "        h = self.decoder_norm(h)\n",
    "        \n",
    "        logits = self.fc_out(h)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            logits_flat = logits.view(-1, self.config.vocab_size)\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                logits_flat, \n",
    "                labels_flat, \n",
    "                ignore_index=0, \n",
    "                label_smoothing=self.label_smoothing\n",
    "            )\n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # Attenuating init for output layers\n",
    "            # Key technique from GPT-2 paper for residual connections stabilization\n",
    "            if isinstance(module, (GroupedQueryAttention, SwiGLU)):\n",
    "                 # Finding output projections: wo in GQA and w2 in SwiGLU\n",
    "                if hasattr(module, 'wo'):\n",
    "                     torch.nn.init.normal_(module.wo.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.num_decoder_layers))\n",
    "                if hasattr(module, 'w2'):\n",
    "                     torch.nn.init.normal_(module.w2.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.num_decoder_layers))\n",
    "    \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def _apply_eps(self, module):\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            module.eps = 1e-5\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, \n",
    "        input_ids, \n",
    "        decoder_input_ids=None,\n",
    "        max_new_tokens=50, \n",
    "        eos_token_id=4,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a sequence using autoregressive decoding with KV caching.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = input_ids.device\n",
    "    \n",
    "        # Encoder pass (once)\n",
    "        h = self.embedding(input_ids)\n",
    "            \n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h, mask=None)\n",
    "        memory = self.encoder_norm(h)\n",
    "    \n",
    "        # Prepare for generation\n",
    "        kv_cache = [(None, None) for _ in range(len(self.decoder_layers))]\n",
    "\n",
    "        if decoder_input_ids is None:\n",
    "            current_tokens = input_ids[:, :1]\n",
    "        else:\n",
    "            current_tokens = decoder_input_ids.to(device)\n",
    "\n",
    "        # \"Warm-up\" the KV cache if a prompt is provided\n",
    "        if current_tokens.size(1) > 1:\n",
    "            prompt_tokens = current_tokens[:, :-1]\n",
    "            h = self.embedding(prompt_tokens)\n",
    "            prompt_mask = torch.zeros(prompt_tokens.shape, dtype=torch.bool, device=device)\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                h, updated_sa_kv = layer(h, memory, src_mask=None, tgt_mask=None, self_attn_kv_cache=None)\n",
    "                kv_cache[i] = updated_sa_kv\n",
    "            \n",
    "            next_token_to_process = current_tokens[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            next_token_to_process = current_tokens.clone()\n",
    "        \n",
    "        generated_ids = [current_tokens]\n",
    "        # Autoregressive generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            h = self.embedding(next_token_to_process)\n",
    "\n",
    "            next_kv_cache = []\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                sa_kv = kv_cache[i]\n",
    "                h, updated_sa_kv = layer(h, memory, src_mask=None, tgt_mask=None, self_attn_kv_cache=sa_kv)\n",
    "                next_kv_cache.append(updated_sa_kv)\n",
    "            kv_cache = next_kv_cache\n",
    "            \n",
    "            h = self.decoder_norm(h)\n",
    "            logits = self.fc_out(h) # shape [b, 1, vocab_size]\n",
    "            \n",
    "            # Sample the next token\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "    \n",
    "            if do_sample:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                if top_k > 0:\n",
    "                    top_k_values, _ = torch.topk(next_token_logits, top_k)\n",
    "                    k_th_value = top_k_values[:, -1].unsqueeze(-1)\n",
    "                    indices_to_remove = next_token_logits < k_th_value\n",
    "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
    "                if top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    next_token_logits[indices_to_remove] = -float('inf')\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "    \n",
    "            generated_ids.append(next_token)\n",
    "            next_token_to_process = next_token.clone()\n",
    "    \n",
    "            if (next_token == eos_token_id).all():\n",
    "                break\n",
    "        \n",
    "        return torch.cat(generated_ids, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "decca02a-c050-472d-a60c-635e31dfa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer_clean.json\")\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "EN_TOKEN_ID = 2\n",
    "DE_TOKEN_ID = 3\n",
    "EOS_TOKEN_ID = 4\n",
    "MASK_TOKEN_ID = 5\n",
    "\n",
    "BATCH_SIZE = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "072c0b7c-252e-4e94-9d13-a52d9c666c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Number of parameters: 148,984,320\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": 30000,\n",
    "    \"d_model\": 640,\n",
    "    \"n_head\": 8,\n",
    "    \"num_kv_heads\": 2,\n",
    "    \"num_encoder_layers\": 10,\n",
    "    \"num_decoder_layers\": 12,\n",
    "}\n",
    "model = ModernEncoderDecoder(**model_config)\n",
    "print(f\"Model initialized. Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01bc8391-4938-4c85-916f-6a6865c8763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading en and de versions of oscar\n",
    "en = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_en',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "de = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_de',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Labeling data by language\n",
    "en = en.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"en\"})\n",
    "de = de.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"de\"})\n",
    "\n",
    "# Shuffling data\n",
    "buffer_size = 10000 \n",
    "seed = 42\n",
    "en = en.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "de = de.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "\n",
    "# interleaving data into one dataset\n",
    "streaming_dataset = interleave_datasets(\n",
    "    [en, de],\n",
    "    probabilities=[0.5, 0.5],\n",
    "    stopping_strategy=\"first_exhausted\",\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# stop words and paterns\n",
    "GENERAL_BAD_PATTERNS = re.compile(\n",
    "    r'''\n",
    "    \\b(\n",
    "        casino|gambling|poker|betting|slots?|roulette|blackjack|baccarat|craps|freespins|bonus|jackpot|wager|no deposit|ohne einzahlung|kostenlos spielen|echtes geld|spielautomaten|spielhalle|spielbank|willkommensbonus|startguthaben|casinospiele|\n",
    "        porn|porno|escort|erotic|hookup|onlyfans|nudes?|camgirls?|sexkontakte|erotik|sexchat|live sex|stripchat|webcamsex|geschlechtsverkehr|selbstbefriedigung|masturbation|pornos|pornhub|xvideos|xnxx|vibrators?|dicks?|cums?|\n",
    "        fast cash|bad credits?|zinsfrei|geld leihen|kredit aufnehmen|ratenzahlung|schnellkredit|binary options?|payday loans?|payday advance|cash advance|short-term loans?|no credit check|guaranteed loan|Kurzzeitkredite?|Minikredite?|Sofortkredite?|Kredit ohne Schufa|schnelles Geld|Geld sofort|\n",
    "        tinder|badoo|parship|elitepartner|lovoo|flirt|verlieben|\n",
    "        test answers?|cheat sheet|homework help|buy answers?|buy exam|abitur lösung|prüfung antworten|examen lösung|\n",
    "        bitcoin|ethereum|blockchain|nft|ico|airdrop|pump and dump|binance|coinbase|kraken|crypto trading|krypto|kryptowährung|\n",
    "        privacy policy|terms of use|terms and conditions|all rights reserved|copyright|impressum|datenschutz|nutzungsbedingungen|alle rechte vorbehalten|cookie policy|agb|rechtliche hinweise|haftungsausschluss|\n",
    "        viagra|levitra|cialis|penis|enlargement|erection|erektionsstörung|potenzmittel|libido|sexualstörung|\n",
    "        weight loss|fat burning|diet pills|appetite suppressant|abnehmen|diätpillen|fettverbrennung|schnell abnehmen|\n",
    "        make money online|side hustle|get rich quick|passives einkommen|geld verdienen|heimarbeit|schnell reich werden|\n",
    "        click here|buy now|order now|free trial|limited offer|jetzt kaufen|hier klicken|jetzt abonnieren|kostenlos testen|nur heute\n",
    "    )\\b\n",
    "    |\n",
    "    -{3,}|={3,}|\\*{3,}|\n",
    "    (?:(?:\\w+\\s*,\\s*){10,}\\w+)\n",
    "    ''',\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "# stop header words\n",
    "BOILERPLATE_HEADER_PATTERNS = re.compile(\n",
    "    r'^(?:\\s*)'\n",
    "    r'(you are not logged in|you do not have permission|access this page|'\n",
    "    r'terms of use|privacy policy|cookies?|all rights reserved|'\n",
    "    r'sign in|log in|register|create an account|register|'\n",
    "    r'skip to content|main navigation|toggle navigation|'\n",
    "    r'select language|choose your region|'\n",
    "    r'cheap|discounts?|easy|billige|rabatte|einfach|'\n",
    "    r'sie sind nicht angemeldet|kein zugriff|'\n",
    "    r'zur hauptnavigation|navigation überspringen|'\n",
    "    r'anmelden|einloggen|registrieren|konto erstellen|'\n",
    "    r'nutzungsbedingungen|datenschutz|cookies?|'\n",
    "    r'alle rechte vorbehalten|sprache auswählen|region wählen|'\n",
    "    r'günstig|rabatt|einfach|schnell|kostenlos|angebot|aktionen|'\n",
    "    r'jetzt anmelden|mehr erfahren|hier klicken|'\n",
    "    r'help|hilfe|assist|unterstützen|call|anrufen|send|senden|respond|antworten|fill|ausfüllen)',\n",
    "\n",
    "    re.IGNORECASE | re.MULTILINE\n",
    ")\n",
    "\n",
    "# One of the many tested filters. No useful patterns were found.\n",
    "def entropy(text):\n",
    "    freqs = {}\n",
    "    for char in text:\n",
    "        freqs[char] = freqs.get(char, 0) + 1\n",
    "\n",
    "    total = sum(freqs.values())\n",
    "    probs = [count / total for count in freqs.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "\n",
    "def filter_texts(example: dict,\n",
    "                 min_num_of_words = 100,\n",
    "                 max_digit_ratio=0.18, \n",
    "                 min_alpha_word_ratio=0.75, \n",
    "                 max_symbol_ratio=0.1, \n",
    "                 header_check_length=200, \n",
    "                 allowed_uppercase_ratio=0.07, \n",
    "                 logging=False) -> bool:\n",
    "    text = example[\"text\"]\n",
    "    lang = example[\"lang\"]\n",
    "    \n",
    "    # Base len filter\n",
    "    if not text or len(text) < 384:\n",
    "        return False\n",
    "\n",
    "    # Stop patterns filter\n",
    "    if GENERAL_BAD_PATTERNS.search(text):\n",
    "        if logging:\n",
    "            print(\"general bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Header stop words filter\n",
    "    text_header = text[:header_check_length]\n",
    "    if BOILERPLATE_HEADER_PATTERNS.search(text_header):\n",
    "        if logging:\n",
    "            print(\"header bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Statistical filters:\n",
    "    num_digits = 0\n",
    "    num_alpha_words = 0\n",
    "    words = text.lower().split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Filter by number of total words\n",
    "    if num_words < min_num_of_words:\n",
    "        if logging:\n",
    "            print(\"num words\")\n",
    "        return False\n",
    "\n",
    "    # Filter ttr\n",
    "    cleaned_words = [word.strip(\".,!?;:`'\\\"\") for word in words]\n",
    "    ttr = len(cleaned_words) / num_words\n",
    "    if num_words < 500:\n",
    "        ttr_threshold = 0.4\n",
    "    elif num_words < 2000:\n",
    "        ttr_threshold = 0.35\n",
    "    else:\n",
    "        ttr_threshold = 0.3\n",
    "        \n",
    "    if ttr < ttr_threshold:\n",
    "        if logging:\n",
    "            print(\"unique words\")\n",
    "        return False\n",
    "        \n",
    "    # Filter by numerical digits and alpha words\n",
    "    for word in words:\n",
    "        num_digits += sum(c.isdigit() for c in word)\n",
    "        if word.replace(\"`\", \"\").replace(\"'\", \"\").isalpha():\n",
    "            num_alpha_words += 1\n",
    "\n",
    "    if (num_digits / len(text)) > max_digit_ratio:\n",
    "        if logging:\n",
    "            print(\"digit words\")\n",
    "        return False\n",
    "\n",
    "    if (num_alpha_words / num_words) < min_alpha_word_ratio:\n",
    "        if logging:\n",
    "            print(\"alpha ratio\")\n",
    "        return False\n",
    "\n",
    "    # Filter by mean word lean\n",
    "    mean_word_len = sum(len(w) for w in words) / num_words\n",
    "    if not (3 < mean_word_len < (15 if lang == 'de' else 12)):\n",
    "        if logging:\n",
    "            print(\"word len\")\n",
    "        return False\n",
    "\n",
    "    # Filter by uppercase ratio\n",
    "    uppercase_chars_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    if uppercase_chars_ratio > allowed_uppercase_ratio:\n",
    "        if logging:\n",
    "            print(\"uppercase ratio\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenizer_test(batch, max_len=320, min_len=256, max_avg_token_id=7000, min_avg_token_len=3.2, max_avg_token_len=5, max_unk_count=3):\n",
    "    # Tokenize and filter\n",
    "    texts = []\n",
    "    # Text standardization\n",
    "    replacements = {\n",
    "        \"\\n\": \"[NL]\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"„\": '\"',\n",
    "        \"’\": \"'\",\n",
    "        \"—\": \"-\",\n",
    "        \"…\": \"...\",\n",
    "        \"`\": \"'\",\n",
    "        \"''\": '\"',\n",
    "        \"$\": \"dollars\",\n",
    "        \"€\": \"euros\",\n",
    "        \"½\": \"1/2\"\n",
    "    }\n",
    "    for i in range(len(batch[\"text\"])):\n",
    "        text = batch[\"text\"][i]\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        texts.append(text)\n",
    "        \n",
    "    # Get encodings\n",
    "    encodings = tokenizer.encode_batch(texts, add_special_tokens=False)\n",
    "    batch_test = [] \n",
    "    batch_token_ids = [None]*len(batch[\"text\"])\n",
    "    \n",
    "    for i, enc in enumerate(encodings):\n",
    "        token_ids = enc.ids\n",
    "        max_tokens_per_text = max_len - 1\n",
    "        # Trim text by max_len and filter by min_len\n",
    "        if len(token_ids) > max_tokens_per_text:\n",
    "            token_ids = token_ids[:max_tokens_per_text]\n",
    "        if len(token_ids) < min_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "\n",
    "        # Filter by unc tokens\n",
    "        unk_count = sum(1 for t in token_ids if t == UNK_TOKEN_ID)\n",
    "        if unk_count >= max_unk_count:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token id\n",
    "        avg_token_id = np.mean(token_ids)\n",
    "        if avg_token_id > max_avg_token_id:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by strange tokens\n",
    "        tokens_as_strings = enc.tokens\n",
    "        if not tokens_as_strings:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token len\n",
    "        avg_token_len = sum(len(t) for t in tokens_as_strings) / len(tokens_as_strings)\n",
    "        if max_avg_token_len < avg_token_len < min_avg_token_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "        # Label text as correct and add batch id\n",
    "        batch_test.append(True)\n",
    "        batch_token_ids[i] = token_ids\n",
    "        \n",
    "    return {\n",
    "        \"tokenizer_test\": batch_test,\n",
    "        \"token_ids\": batch_token_ids\n",
    "    }\n",
    "\n",
    "def filter_by_tokenizer_test(example):\n",
    "    # Separated from tokenizer_test because it allows tokenizing texts in batches and returning token_ids.\n",
    "    return example[\"tokenizer_test\"]\n",
    "\n",
    "streaming_dataset = streaming_dataset.filter(filter_texts)\n",
    "streaming_dataset = streaming_dataset.map(tokenizer_test, batched=True, batch_size=BATCH_SIZE*8,)\n",
    "streaming_dataset = streaming_dataset.filter(filter_by_tokenizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3535dd49-c3ec-4b66-845f-86b15c6188a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Span parametrs\n",
    "MEAN_SPAN_LENGTH = 12.0\n",
    "MIN_SPAN_LENGTH = 2\n",
    "MAX_SPAN_LENGTH = 40\n",
    "p = 1.0 / MEAN_SPAN_LENGTH\n",
    "def generate_span_length():\n",
    "    # Geometric distribution of spans\n",
    "    length = np.random.geometric(p=p)\n",
    "    length = max(length, MIN_SPAN_LENGTH)\n",
    "    length = min(length, MAX_SPAN_LENGTH)\n",
    "    return length\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, mask_prob, copy_prob, en_token_id=2, de_token_id=3, eos_token_id=4, pad_token_id=0, mask_token_id=5,):\n",
    "        self.mask_prob = mask_prob\n",
    "        self.copy_prob = copy_prob # tested feature\n",
    "\n",
    "        self.EN_TOKEN_ID = en_token_id\n",
    "        self.DE_TOKEN_ID = de_token_id\n",
    "        self.EOS_TOKEN_ID = eos_token_id\n",
    "        self.PAD_TOKEN_ID = pad_token_id\n",
    "        self.MASK_TOKEN_ID = mask_token_id\n",
    "\n",
    "        self.all_ids = set()\n",
    "\n",
    "    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_input_ids = []\n",
    "        batch_attention_mask = []\n",
    "        batch_labels = []\n",
    "        batch_decoder_input_ids = []\n",
    "        batch_decoder_attention_mask = []\n",
    "\n",
    "        # copy_prob may stabilize training process, but it didn`t need as practice had shown\n",
    "        actual_mask_prob = 0.0 if random.random() <= self.copy_prob else self.mask_prob\n",
    "        # Dynamic batch size can speed up training if none of the batch items len == max_len\n",
    "        batch_len = 0\n",
    "        for item in batch:\n",
    "            if len(item[\"token_ids\"]) > batch_len:\n",
    "                batch_len = len(item[\"token_ids\"])\n",
    "\n",
    "        #print(f'Batch len: {batch_len}. Ids: {item[\"token_ids\"][0:5]}...')\n",
    "        for item in batch:\n",
    "            token_ids = item[\"token_ids\"]\n",
    "            lang = item[\"lang\"]\n",
    "            \n",
    "            # process token ids into encoder input, decoder input and output.\n",
    "            lang_token = self.EN_TOKEN_ID if lang == \"en\" else self.DE_TOKEN_ID\n",
    "\n",
    "            input_ids = [lang_token] + token_ids + [self.EOS_TOKEN_ID]\n",
    "            decoder_ids = [lang_token] + token_ids\n",
    "            labels = token_ids + [self.EOS_TOKEN_ID]\n",
    "            \n",
    "            padding_length = batch_len - len(token_ids)\n",
    "            input_ids.extend([self.PAD_TOKEN_ID] * (padding_length-1))\n",
    "            decoder_ids.extend([self.PAD_TOKEN_ID] * padding_length)\n",
    "            labels.extend([self.PAD_TOKEN_ID] * padding_length)\n",
    "            \n",
    "            decoder_attention_mask = [1 if tid != self.PAD_TOKEN_ID else 0 for tid in decoder_ids]\n",
    "            attention_mask = [1 if tid != self.PAD_TOKEN_ID else 0 for tid in input_ids]\n",
    "\n",
    "            # Add processed items to batch\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_mask.append(attention_mask)\n",
    "            batch_labels.append(labels)\n",
    "            batch_decoder_input_ids.append(decoder_ids)\n",
    "            batch_decoder_attention_mask.append(decoder_attention_mask)\n",
    "\n",
    "        # Maksing spans\n",
    "        input_ids_np = np.array(batch_input_ids, dtype=np.int32)\n",
    "        rand = np.random.rand(*input_ids_np.shape)\n",
    "        can_be_masked = (input_ids_np != self.PAD_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.EN_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.DE_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.EOS_TOKEN_ID)\n",
    "\n",
    "        mask_selection = (rand < actual_mask_prob)\n",
    "        force = 0\n",
    "        for i in range(len(mask_selection)):\n",
    "            for j in range(len(mask_selection[i])):\n",
    "                if force>0:\n",
    "                    mask_selection[i][j]=True\n",
    "                    force-=1\n",
    "                else:\n",
    "                    if mask_selection[i][j]==True:\n",
    "                        force = generate_span_length()\n",
    "                \n",
    "        mask_selection = mask_selection & can_be_masked\n",
    "        mask_sum = sum([1 if i else 0 for i in mask_selection[0]])\n",
    "        #print(f\"tok_len = {batch_len}, mask_sum = {mask_sum}, mask% = {1/(batch_len/mask_sum)}\")\n",
    "        # Unite several nearby mask tokens ([1], [MASK], [MASK], [MASK], [1] -> [1], [MASK], [1])\n",
    "        input_ids_np[mask_selection] = self.MASK_TOKEN_ID\n",
    "        batch_input_ids = input_ids_np.tolist()\n",
    "        fixed_batch_input_ids = []\n",
    "        fixed_batch_attention_mask = []\n",
    "        for ids, mask in zip(batch_input_ids, batch_attention_mask):\n",
    "            fixed_ids = []\n",
    "            pads = []\n",
    "            fixed_mask = []\n",
    "            span_sterted = False\n",
    "            for i in ids:\n",
    "                if span_sterted:\n",
    "                    if i != self.MASK_TOKEN_ID:\n",
    "                        span_sterted = False\n",
    "                        fixed_ids.append(i)\n",
    "                    else:\n",
    "                        pads.append(self.PAD_TOKEN_ID)\n",
    "                elif i == self.MASK_TOKEN_ID:\n",
    "                    fixed_ids.append(i)\n",
    "                    span_sterted = True\n",
    "                else:\n",
    "                    fixed_ids.append(i)\n",
    "            fixed_mask = mask[len(mask)-len(pads):] = [0]*len(pads)\n",
    "            fixed_ids = fixed_ids + pads\n",
    "            fixed_batch_input_ids.append(fixed_ids)\n",
    "            fixed_batch_attention_mask.append(fixed_mask)\n",
    "                \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(fixed_batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention_mask, dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(batch_decoder_input_ids, dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(batch_decoder_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f000c73a-081a-43d9-900e-f94138ee49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realization of dynamic masking. Needed to encourage decoder.\n",
    "def step_scheduler(start_pos, end_pos, max_steps):\n",
    "    x = np.linspace(0, np.pi, max_steps*2)\n",
    "    cosine_decay = 0.5 * (np.cos(x) + 1)\n",
    "    cosine_curve = end_pos + (start_pos - end_pos) * cosine_decay[max_steps:]*2\n",
    "    return cosine_curve\n",
    "    \n",
    "class DataCollatorDynamic:\n",
    "    def __init__(self, start_prob, end_prob, steps, en_token_id=2, de_token_id=3, eos_token_id=4, pad_token_id=0, mask_token_id=5,):\n",
    "        self.schedule = step_scheduler(start_prob, end_prob, steps)\n",
    "        \n",
    "        self.EN_TOKEN_ID = en_token_id\n",
    "        self.DE_TOKEN_ID = de_token_id\n",
    "        self.EOS_TOKEN_ID = eos_token_id\n",
    "        self.PAD_TOKEN_ID = pad_token_id\n",
    "        self.MASK_TOKEN_ID = mask_token_id\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        self.all_ids = set()\n",
    "\n",
    "    def __call__(self, batch: List[Dict], step: int | None = None) -> Dict[str, torch.Tensor]:\n",
    "        batch_input_ids = []\n",
    "        batch_attention_mask = []\n",
    "        batch_labels = []\n",
    "        batch_decoder_input_ids = []\n",
    "        batch_decoder_attention_mask = []\n",
    "\n",
    "        # Identify current step and mask_prob\n",
    "        try:\n",
    "            if not step:\n",
    "                step = self.step\n",
    "                self.step += 1\n",
    "            actual_mask_prob = self.schedule[step]\n",
    "        except IndexError:\n",
    "            actual_mask_prob = self.schedule[-1]\n",
    "\n",
    "        #if step % 36 == 0:\n",
    "            #print(f\"step: {step}, actual_mask_prob: {actual_mask_prob}\")\n",
    "            \n",
    "        # Dynamic batch_len\n",
    "        batch_len = 0\n",
    "        for item in batch:\n",
    "            if len(item[\"token_ids\"]) > batch_len:\n",
    "                batch_len = len(item[\"token_ids\"])\n",
    "\n",
    "        #print(f'Batch len: {batch_len}. Ids: {item[\"token_ids\"][0:5]}...')\n",
    "        for item in batch:\n",
    "            token_ids = item[\"token_ids\"]\n",
    "            lang = item[\"lang\"]\n",
    "            \n",
    "            # process token ids into encoder input, decoder input and output.\n",
    "            lang_token = self.EN_TOKEN_ID if lang == \"en\" else self.DE_TOKEN_ID\n",
    "\n",
    "            input_ids = [lang_token] + token_ids + [self.EOS_TOKEN_ID]\n",
    "            decoder_ids = [lang_token] + token_ids\n",
    "            labels = token_ids + [self.EOS_TOKEN_ID]\n",
    "            \n",
    "            padding_length = batch_len - len(token_ids)\n",
    "            input_ids.extend([self.PAD_TOKEN_ID] * (padding_length-1))\n",
    "            decoder_ids.extend([self.PAD_TOKEN_ID] * padding_length)\n",
    "            labels.extend([self.PAD_TOKEN_ID] * padding_length)\n",
    "            \n",
    "            decoder_attention_mask = [1 if tid != self.PAD_TOKEN_ID else 0 for tid in decoder_ids]\n",
    "            attention_mask = [1 if tid != self.PAD_TOKEN_ID else 0 for tid in input_ids]\n",
    "\n",
    "            # Add processed items to batch\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_mask.append(attention_mask)\n",
    "            batch_labels.append(labels)\n",
    "            batch_decoder_input_ids.append(decoder_ids)\n",
    "            batch_decoder_attention_mask.append(decoder_attention_mask)\n",
    "\n",
    "        # Maksing spans\n",
    "        input_ids_np = np.array(batch_input_ids, dtype=np.int32)\n",
    "        rand = np.random.rand(*input_ids_np.shape)\n",
    "        can_be_masked = (input_ids_np != self.PAD_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.EN_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.DE_TOKEN_ID) & \\\n",
    "                        (input_ids_np != self.EOS_TOKEN_ID)\n",
    "\n",
    "        mask_selection = (rand < actual_mask_prob)\n",
    "        force = 0\n",
    "        for i in range(len(mask_selection)):\n",
    "            for j in range(len(mask_selection[i])):\n",
    "                if force>0:\n",
    "                    mask_selection[i][j]=True\n",
    "                    force-=1\n",
    "                else:\n",
    "                    if mask_selection[i][j]==True:\n",
    "                        force = generate_span_length()\n",
    "                \n",
    "        mask_selection = mask_selection & can_be_masked\n",
    "        mask_sum = sum([1 if i else 0 for i in mask_selection[0]])\n",
    "        #print(f\"tok_len = {batch_len}, mask_sum = {mask_sum}, mask% = {1/(batch_len/mask_sum)}\")\n",
    "        \n",
    "        # Unite several nearby mask tokens ([1], [MASK], [MASK], [MASK], [1] -> [1], [MASK], [1])\n",
    "        input_ids_np[mask_selection] = self.MASK_TOKEN_ID\n",
    "        batch_input_ids = input_ids_np.tolist()\n",
    "        fixed_batch_input_ids = []\n",
    "        fixed_batch_attention_mask = []\n",
    "        for ids, mask in zip(batch_input_ids, batch_attention_mask):\n",
    "            fixed_ids = []\n",
    "            pads = []\n",
    "            fixed_mask = []\n",
    "            span_sterted = False\n",
    "            for i in ids:\n",
    "                if span_sterted:\n",
    "                    if i != self.MASK_TOKEN_ID:\n",
    "                        span_sterted = False\n",
    "                        fixed_ids.append(i)\n",
    "                    else:\n",
    "                        pads.append(self.PAD_TOKEN_ID)\n",
    "                elif i == self.MASK_TOKEN_ID:\n",
    "                    fixed_ids.append(i)\n",
    "                    span_sterted = True\n",
    "                else:\n",
    "                    fixed_ids.append(i)\n",
    "            fixed_mask = mask[len(mask)-len(pads):] = [0]*len(pads)\n",
    "            fixed_ids = fixed_ids + pads\n",
    "            fixed_batch_input_ids.append(fixed_ids)\n",
    "            fixed_batch_attention_mask.append(fixed_mask)\n",
    "                \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(fixed_batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention_mask, dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(batch_decoder_input_ids, dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(batch_decoder_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "263f6deb-5e5d-4b6b-b743-296d72574fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollator(mask_prob=0.023,#0.023, \n",
    "                             copy_prob=0)\n",
    "\n",
    "data_collator_dynamic = DataCollatorDynamic(\n",
    "                             start_prob=0.08,\n",
    "                             end_prob=0.023,#0.023,\n",
    "                             steps=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0469eaa-ddd3-4f92-b959-5a97ff23c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usefull logs\n",
    "class AdvancedDiagnosticsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        tokenizer = kwargs.get(\"tokenizer\")\n",
    "        inputs = kwargs.get(\"inputs\")\n",
    "        if model is None or wandb.run is None:\n",
    "            return\n",
    "\n",
    "        step = state.global_step\n",
    "\n",
    "        # Loss & perplexity\n",
    "        if logs and \"loss\" in logs:\n",
    "            loss_val = logs[\"loss\"]\n",
    "            perplexity = torch.exp(torch.tensor(loss_val)).item()\n",
    "            run.log({\n",
    "                \"loss\": loss_val,\n",
    "                \"perplexity\": perplexity,\n",
    "            }, step=step)\n",
    "\n",
    "        # Learning rate\n",
    "        try:\n",
    "            lr = kwargs[\"optimizer\"].param_groups[0][\"lr\"]\n",
    "            run.log({\"learning_rate\": lr}, step=step)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Weight stats\n",
    "        weights = model.fc_out.weight.detach().cpu().float()\n",
    "        run.log({\n",
    "            \"weights/fc_out_mean\": weights.mean().item(),\n",
    "            \"weights/fc_out_std\": weights.std().item(),\n",
    "            \"weights/fc_out_hist\": wandb.Histogram(weights.numpy())\n",
    "        }, step=step)\n",
    "\n",
    "        # Sparsity\n",
    "        zero_ratio = (weights.abs() < 1e-5).float().mean().item()\n",
    "        run.log({\"weights/fc_out_sparsity\": zero_ratio}, step=step)\n",
    "        \n",
    "diag_callback = AdvancedDiagnosticsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a84c3ebe-d336-4ed6-970a-3cc04690d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    max_steps=3500,\n",
    "    #num_train_epochs=1000,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=300,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=300,\n",
    "    save_total_limit=8,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"aot_eager\",\n",
    "    save_safetensors = False,\n",
    "    dataloader_num_workers=2,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c68bc4d-796f-49f8-b8ba-0bfdabc1c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was playing with encoder during experiments\n",
    "for layer in model.encoder_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "216005c0-537b-4081-9c75-b5b02d89dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"EDT-lm\", name=\"Pretrain\")\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=streaming_dataset,\n",
    "    data_collator=data_collator_dynamic,\n",
    "    callbacks=[diag_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ee573df-937f-40fd-9271-4e935de3404c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = trainer.train()#resume_from_checkpoint=\"results/checkpoint-900\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fa8ed15-3924-4126-bfaa-a228af8ea27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c46f7fcd-91eb-40b8-918a-bcb246a718a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"model_weights.pth\", map_location='cuda')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b27eacf-6584-4a03-a8e6-5878cf4bd413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"checkpoints/checkpoint-3315-deen-pretrain/pytorch_model.bin\", map_location='cuda')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9afdf2-3823-4865-9f33-c5844d1bbcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"results_translate/checkpoint-4242/pytorch_model.bin\", map_location='cuda')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aef58c7b-bbf6-4c04-baa5-022a4f6cf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classsic top_p sampling\n",
    "@torch.no_grad()\n",
    "def top_p_decode(\n",
    "    model, tokenizer, input_ids, start_tokens, attention_mask=None, max_new_tokens=50,\n",
    "    eos_token_id=4, top_p=0.3, temperature=1.0,\n",
    "):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Encoder pass\n",
    "    encoder_outputs = model.embedding(input_ids)\n",
    "    if attention_mask is not None:\n",
    "        attn_mask = (attention_mask == 1)\n",
    "        attn_mask = attn_mask[:, None, :] | attn_mask[:, :, None]\n",
    "    else:\n",
    "        attn_mask = None\n",
    "\n",
    "    for layer in model.encoder_layers:\n",
    "        encoder_outputs = layer(encoder_outputs, mask=attn_mask)\n",
    "    memory = model.encoder_norm(encoder_outputs)\n",
    "\n",
    "    decoder_input_ids = torch.tensor([start_tokens], device=device)\n",
    "\n",
    "    raw_probabilities = []\n",
    "    tempered_probabilities = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tgt_embeddings = model.embedding(decoder_input_ids)\n",
    "\n",
    "        for layer in model.decoder_layers:\n",
    "            tgt_embeddings, _ = layer(\n",
    "                tgt_embeddings,\n",
    "                memory,\n",
    "                src_mask=None,\n",
    "                tgt_mask=None,\n",
    "                self_attn_kv_cache=None\n",
    "            )\n",
    "\n",
    "        h = model.decoder_norm(tgt_embeddings)\n",
    "        logits = model.fc_out(h)  # [batch, seq_len, vocab]\n",
    "\n",
    "        raw_next_token_logits = logits[:, -1, :]\n",
    "        raw_probs_dist = F.softmax(raw_next_token_logits, dim=-1)\n",
    "\n",
    "        next_token_logits = raw_next_token_logits / temperature\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        sorted_mask = cumulative_probs > top_p\n",
    "        sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "        sorted_mask[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_mask.scatter(1, sorted_indices, sorted_mask)\n",
    "        next_token_logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "        final_probs_dist = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(final_probs_dist, num_samples=1)\n",
    "\n",
    "        chosen_raw_prob = raw_probs_dist[0, next_token.item()].item()\n",
    "        raw_probabilities.append(chosen_raw_prob)\n",
    "\n",
    "        chosen_final_prob = final_probs_dist[0, next_token.item()].item()\n",
    "        tempered_probabilities.append(chosen_final_prob)\n",
    "\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "        if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "            break\n",
    "            \n",
    "    return decoder_input_ids, raw_probabilities, tempered_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb19d622-0dce-460f-acc3-ca7312983483",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(streaming_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9921b2fb-9d7a-43e1-86ab-4a7627b6afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorDynamic(start_prob=0.3, end_prob=0.023, steps=40000)\n",
    "data_collator = DataCollator(mask_prob=0.023, copy_prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c986b8b9-9552-4c04-aeed-e8cdbc0516c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_item = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da364d80-f20f-49cd-9e33-1d32194ebb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"I have a cat. His name is Leo. He is a big, fluffy cat with bright green eyes. Leo loves to sleep in the sun. He also likes to play with a small red ball. Every morning, he waits by his bowl for his favorite food. When I pet him, he starts to purr loudly. He is a very good and gentle friend.\"\n",
    "#raw_item[\"token_ids\"] = tokenizer.encode(text).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ed62855-f234-49c4-b0c9-759a28c51ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data_collator([raw_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a82c0b2-a12c-4860-8ed3-a3339d5e37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base sequence:\n",
      "[SOSE] We are here to help you. If you have need of our services, please call us, day or night. Or, if you prefer, you can fill out the form on the right.\n",
      " Are you thinking about pre-planning your funeral? Pre-planning is the best way to choose how you're remembered, to ease the emotional and financial burden on your loved ones, to protect yourself from rising funeral costs, and to let your family know your final wishes.\n",
      " If you are looking for information[MASK] for a loved[MASK] been entrusted to our care, you can use the form below to narrow down your search.\n",
      " Ordering flowers from our site ensures that your order will reach us or the family in a timely manner, and your gesture of support will remain acknowledged in the Book of Memories[UNK] for future generations. We only work with local florists so we can maintain the sense of urgency and quality of your selections. We thank you for helping to support the family during their time of need, and will fondly remember your kind gesture.\n",
      " We respect the sensitivity and dignity that comes with ordering tribute gifts from the funeral home. It is for this reason that we have attempted to design our online Sympathy Store with the utmost respect to the family and the deceased.[MASK] welcome any comments or suggestions you might have to help us serve our communities better[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "Denoised sequence (top-p autoregresive search model1)\n",
      "[SOSE] We are here to help you. If you have need of our services, please call us, day or night. Or, if you prefer, you can fill out the form on the right.\n",
      " Are you thinking about pre-planning your funeral? Pre-planning is the best way to choose how you're remembered, to ease the emotional and financial burden on your loved ones, to protect yourself from rising funeral costs, and to let your family know your final wishes.\n",
      " If you are looking for information and advice regarding a funeral shop, contact our staff to call and send you an email for a loved one to help. If you have been entrusted to our care, you can use the form below to narrow down your search.\n",
      " Ordering flowers from our site ensures that your order will reach us or the family in a timely manner, and your gesture of support will remain acknowledged in the Book of Memories[UNK] for future generations. We only work with local florists so we can maintain the sense of urgency and quality of your selections. We thank you for helping to support the family during their time of need, and will fondly remember your kind gesture.\n",
      " We respect the sensitivity and dignity that comes with ordering tribute gifts from the funeral home. It is for this reason that we have attempted to design our online Sympathy Store with the utmost respect to the family and the deceased\n"
     ]
    }
   ],
   "source": [
    "print(\"Base sequence:\")\n",
    "print(tokenizer.decode(item[\"input_ids\"].tolist()[0], skip_special_tokens=False).replace(\"[NL]\", \"\\n\"))\n",
    "\n",
    "output_ids, raw_probs, probs = top_p_decode(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids=item[\"input_ids\"][0].unsqueeze(0),\n",
    "    #attention_mask=item.get(\"attention_mask\"),\n",
    "    max_new_tokens=300,\n",
    "    eos_token_id=4,\n",
    "    top_p=0.8, \n",
    "    temperature=1,\n",
    "    start_tokens=tokenizer.encode(\"[SOSE]\").ids\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Denoised sequence (top-p autoregresive search model1)\")\n",
    "out_list = output_ids[0].tolist()\n",
    "decoded_text = tokenizer.decode(out_list, skip_special_tokens=False).replace(\"[NL]\", \"\\n\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cb74c11-7a43-426f-b16c-ac76916c5f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for prob, raw_prob, token in zip(probs, raw_probs, out_list):\n",
    "#   print(prob, raw_prob, token, tokenizer.decode([token], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b6a65-9e01-4cc8-ad62-8034d23db5fd",
   "metadata": {},
   "source": [
    "## fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ab66ad1-1ee0-47d7-bc7c-da455f86df23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "with open(\"translations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part1 = json.load(f)\n",
    "with open(\"translations2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part2 = json.load(f)\n",
    "with open(\"translations3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part3 = json.load(f)\n",
    "with open(\"translations4.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part4 = json.load(f)\n",
    "with open(\"translations5.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part5 = json.load(f)\n",
    "with open(\"translations6.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part6 = json.load(f)\n",
    "    \n",
    "text_pairs = {\n",
    "    \"en\": data_part1[\"en\"] + data_part2[\"en\"] + data_part3[\"en\"] + data_part4[\"en\"] + data_part5[\"en\"] + data_part6[\"en\"],\n",
    "    \"de\": data_part1[\"de\"] + data_part2[\"de\"] + data_part3[\"de\"] + data_part4[\"de\"] + data_part5[\"de\"] + data_part6[\"de\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea9f0334-b284-4dff-bb4d-cb07e68cd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79446"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_pairs[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4731e84a-ba39-402f-a3fa-e98f5e143e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorTranslate:\n",
    "    def __init__(self, translate_from, pad_token_id=0):\n",
    "        self.translate_from = translate_from\n",
    "        self.PAD_TOKEN_ID = pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_input_ids = []\n",
    "        batch_attention_mask = []\n",
    "        batch_labels = []\n",
    "        batch_decoder_input_ids = []\n",
    "        batch_decoder_attention_mask = []\n",
    "\n",
    "        batch_len = 0\n",
    "        for item in batch:\n",
    "            if len(item['en_ids']) > batch_len:\n",
    "                batch_len = len(item['en_ids'])\n",
    "            if len(item['de_ids']) > batch_len:\n",
    "                batch_len = len(item['de_ids'])\n",
    "\n",
    "        #print(f'Batch len: {batch_len}')\n",
    "        for item in batch:\n",
    "            translate_to = [\"de\", \"en\"]\n",
    "            if self.translate_from:\n",
    "                translate_from = self.translate_from\n",
    "            else:\n",
    "                translate_from = random.choice(translate_to)\n",
    "            translate_to.remove(translate_from)\n",
    "            \n",
    "            from_ids = item[f\"{translate_from}_ids\"]\n",
    "            to_ids = item[f\"{translate_to[0]}_ids\"]\n",
    "\n",
    "            labels = to_ids[1:]\n",
    "            decoder_ids = to_ids[:-1]\n",
    "\n",
    "            pad_length_from = batch_len - len(from_ids)\n",
    "            pad_length_to = batch_len - len(decoder_ids)\n",
    "\n",
    "            from_ids.extend([self.PAD_TOKEN_ID] * pad_length_from)\n",
    "            decoder_ids.extend([self.PAD_TOKEN_ID] * pad_length_to)\n",
    "            labels.extend([self.PAD_TOKEN_ID] * pad_length_to)\n",
    "\n",
    "            attention_mask = [1 if token != self.PAD_TOKEN_ID else 0 for token in from_ids]\n",
    "            decoder_attention_mask = [1 if token != self.PAD_TOKEN_ID else 0 for token in decoder_ids]\n",
    "\n",
    "            batch_input_ids.append(from_ids)\n",
    "            batch_attention_mask.append(attention_mask)\n",
    "            batch_labels.append(labels)\n",
    "            batch_decoder_input_ids.append(decoder_ids)\n",
    "            batch_decoder_attention_mask.append(decoder_attention_mask)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention_mask, dtype=torch.long),\n",
    "            \"decoder_input_ids\": torch.tensor(batch_decoder_input_ids, dtype=torch.long),\n",
    "            \"decoder_attention_mask\": torch.tensor(batch_decoder_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8f33640-f523-4910-9ef2-e34757be5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_translate = DataCollatorTranslate(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ffc4504-d99c-4f99-8af4-7a247fa6a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_translate = TrainingArguments(\n",
    "    output_dir=\"./results_translate\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=45,\n",
    "    save_strategy=\"epoch\",\n",
    "    #save_steps=300,\n",
    "    save_total_limit=8,\n",
    "    logging_strategy=\"epoch\",\n",
    "    #logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    #eval_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"aot_eager\",\n",
    "    save_safetensors = False,\n",
    "    dataloader_num_workers=8,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfd80c3f-d15d-4c1c-982a-ddf3d72f6dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b19f4e2cfa84221ac53fc656621b210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b4c0d8d7c748d985ced24e2b54dc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/79446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch, tokenizer=tokenizer):\n",
    "    texts_en = []\n",
    "    texts_de = []\n",
    "    # Texts standartization\n",
    "    replacements = {\n",
    "        \"\\n\": \"[NL]\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"„\": '\"',\n",
    "        \"’\": \"'\",\n",
    "        \"‘\": \"'\",\n",
    "        \"—\": \"-\",\n",
    "        \"–\": \"-\",\n",
    "        \"…\": \"...\",\n",
    "        \"`\": \"'\",\n",
    "        \"''\": '\"',\n",
    "        \"$\": \"(dollars)\",\n",
    "        \"€\": \"(euros)\",\n",
    "        \"½\": \"1(backslash)2\",\n",
    "        \"²\": \"(caret)2\",\n",
    "        \"/\": \"(backslash)\",\n",
    "        \"^\": \"(caret)\",\n",
    "        \"″\": '\"',\n",
    "        \"%\": \"(percent)\",\n",
    "        \"=\": \"(equals)\",\n",
    "    }\n",
    "    for i in range(len(batch[\"en\"])):\n",
    "        text_en = batch[\"en\"][i]\n",
    "        text_de = batch[\"de\"][i]\n",
    "        for old, new in replacements.items():\n",
    "            text_en = text_en.replace(old, new)\n",
    "            text_de = text_de.replace(old, new)\n",
    "        texts_en.append(text_en)\n",
    "        texts_de.append(text_de)\n",
    "\n",
    "    # Tokenize\n",
    "    en_encodings = tokenizer.encode_batch(texts_en)\n",
    "    de_encodings = tokenizer.encode_batch(texts_de)\n",
    "\n",
    "    en_ids = [[2] + encoding.ids + [4] for encoding in en_encodings]\n",
    "    de_ids = [[3] + encoding.ids + [4] for encoding in de_encodings]\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"en_ids\": en_ids,\n",
    "        \"de_ids\": de_ids\n",
    "    }\n",
    "        \n",
    "def filter_translation(item):\n",
    "    # min len in tokens of each text\n",
    "    min_len = 6\n",
    "    if len(item[\"en_ids\"]) < min_len or len(item[\"de_ids\"]) < min_len:\n",
    "        return False\n",
    "\n",
    "    # max len in tokens of each text\n",
    "    max_len = 320\n",
    "    if len(item[\"en_ids\"]) > max_len or len(item[\"de_ids\"]) > max_len:\n",
    "        return False\n",
    "\n",
    "    # ratio of tokens in pair\n",
    "    min_ratio = 0.85\n",
    "    if min(len(item[\"en_ids\"]), len(item[\"de_ids\"])) / max(len(item[\"en_ids\"]), len(item[\"de_ids\"])) < min_ratio:\n",
    "        return False\n",
    "\n",
    "    # allowed unk tokens\n",
    "    unk_tok_id = 1\n",
    "    max_unk_tok = 1\n",
    "    if item[\"en_ids\"].count(unk_tok_id) > max_unk_tok or item[\"de_ids\"].count(unk_tok_id) > max_unk_tok:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "translate_dataset = HFDataset.from_dict(text_pairs).shuffle(seed=42).map(tokenize, batched=True, batch_size=BATCH_SIZE).filter(filter_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cfdb60c-b48e-4e86-bce5-4b48f08e3d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de', 'en_ids', 'de_ids'],\n",
       "        num_rows: 62974\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de', 'en_ids', 'de_ids'],\n",
       "        num_rows: 3315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = translate_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d60bb9c-48dd-408c-bf06-0eaf2208dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translate_dataset = datasets[\"train\"]\n",
    "test_translate_dataset = datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e068b-47e3-4a16-b038-bd553d8096d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run = wandb.init(project=\"EDT-lm\", name=\"FineTune\")\n",
    "trainer_fine_tune = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_translate,\n",
    "    train_dataset=train_translate_dataset,\n",
    "    eval_dataset=test_translate_dataset,\n",
    "    data_collator=data_collator_translate,\n",
    "    #callbacks=[diag_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae3dc6-f740-4751-abba-74353a4450be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_fine_tune.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "477c5815-75db-4aba-9ab2-77d573c7c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_translate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d749730c-e416-468d-994e-5693b538e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iterator)\n",
    "item = data_collator_translate([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d96206a6-0612-4ad4-b61a-5a4eed426ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,   233,    16,  2987,   826,   870, 23438,    10,  7599,    65,\n",
       "           874,    17,  1359,   669,     7,    42,   111,    15, 22777,    17,\n",
       "          1012,   390, 12025,  9056,  3001,  5271,  5566,    17,     7,    10,\n",
       "           332,    45,    86, 11673,    11,  3757,   200,   458,    14,    25,\n",
       "           790, 10725,    11,    24,  4126,   149,   633,  9600,     7,   141,\n",
       "            93,    15, 11806,  5326,     7,    15,   870,    62, 16710,     7,\n",
       "         16400,  6576,    10,  2120,   208,    65,   115,     7,   214, 17508,\n",
       "             9,    16,    62,  7214,    11, 15944,    18,    24,  4892,    12,\n",
       "            15,   423,    31,   717, 14766,    24, 10230,     8,    10,     4]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a4fe032-40dc-4aad-9953-40676d66fdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"results_translate/checkpoint-4848/pytorch_model.bin\", map_location='cuda')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61ba30d8-f907-4fe8-ac99-2b90ecee72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a1 = \"[SOSD] Ich bin Anna. Ich fahre nach Berlin. Ich buche ein Hotel. Das Hotel ist gut. Berlin ist groß. Ich mag Berlin. [EOS]\"\n",
    "text_a2 = \"[SOSD] Hallo, mein Name ist Anna. Nächste Woche fliege ich nach Berlin, weil ich die Stadt besuchen will. Ich habe schon ein Hotelzimmer gebucht. Ich möchte die Sehenswürdigkeiten sehen und vielleicht in einem Restaurant essen. Kannst du mir helfen? [EOS]\"\n",
    "\n",
    "text_b1 = \"[SOSD] Ich bereite gerade meine Reise nach Berlin vor und freue mich schon sehr darauf. Obwohl das Wetter nicht immer perfekt ist, hoffe ich, dass wir viele Parks erkunden können. Mir wurde gesagt, dass man unbedingt das Brandenburger Tor besuchen sollte. Falls du Zeit hast, könnten wir uns vielleicht treffen, um gemeinsam die Stadt zu entdecken. [EOS]\"\n",
    "text_b2 = \"[SOSD] Bei der Planung meiner Berlin-Reise lege ich besonderen Wert darauf, nicht nur die touristischen Hauptattraktionen abzuklappern, sondern auch einen authentischen Eindruck vom Leben in den verschiedenen Stadtteilen zu bekommen. Es wäre schade, wenn man die kulturelle Vielfalt, die Berlin auszeichnet, verpassen würde. Deshalb habe ich vor, mich abseits der ausgetretenen Pfade zu bewegen und die Stadt auf eigene Faust zu erkunden. [EOS]\"\n",
    "\n",
    "text_c1 = \"[SOSD] Die Faszination Berlins ergibt sich aus dem ständigen Spannungsfeld zwischen seiner turbulenten Vergangenheit und seiner dynamischen Gegenwart. Um die Stadt wirklich zu begreifen, genügt es nicht, die Überreste der Mauer zu betrachten; man muss sich mit den vielschichtigen Narrativen auseinandersetzen, die das heutige Stadtbild prägen. Der Facettenreichtum der Metropole offenbart sich erst bei der eingehenden Beschäftigung mit ihren subkulturellen Strömungen und der permanenten urbanen Transformation. [EOS]\"\n",
    "text_c2 = \"[SOSD] Die epistemologische Auseinandersetzung mit Berlin als Palimpsest der deutschen Geschichte erfordert eine hohe Ambiguitätstoleranz seitens des Betrachters. Die diskursive Hegemonie etablierter Gedenkorte wird zunehmend durch partizipative Geschichtsnarrative dekonstruiert, was die per se prekäre Verfasstheit kollektiver Identität perpetuiert und zu einer fortwährenden Neuaushandlung des städtischen Selbstverständnisses führt. [EOS]\"\n",
    "\n",
    "text_free = \"[SOSD] Hey, was geht ab? Ich hab mir überlegt, wir könnten heute Abend ins Kino gehen. Der neue Film soll mega krass sein, oder? Ich hab aber keine Ahnung, worum es geht. Soll halt gut sein. Bei dir läuft's ja eh. Also, sag Bescheid! [EOS]\"\n",
    "\n",
    "item[\"input_ids\"] = torch.tensor([tokenizer.encode(text_b1).ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f9d26fc-ba32-49d3-afd7-da56914635c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base sequence:\n",
      "[SOSD] Ich bereite gerade meine Reise nach Berlin vor und freue mich schon sehr darauf. Obwohl das Wetter nicht immer perfekt ist, hoffe ich, dass wir viele Parks erkunden können. Mir wurde gesagt, dass man unbedingt das Brandenburger Tor besuchen sollte. Falls du Zeit hast, könnten wir uns vielleicht treffen, um gemeinsam die Stadt zu entdecken. [EOS]\n",
      "\n",
      "Tokens generated: 60\n",
      "Execution time: 1.2906 second\n",
      "Tokens generated: 60\n",
      "Execution time: 1.2155 second\n",
      "\n",
      "Denoised sequence (top-p autoregresive search)\n",
      "[SOSE] I am currently planning my trip to Berlin and am already very excited about it. Although the weather isn't always perfect, I hope we can explore many parks. I was told that you should definitely visit the Brandenburg Gate. If you have time, we might meet together to discover the city together.[EOS]\n"
     ]
    }
   ],
   "source": [
    "print(\"Base sequence:\")\n",
    "print(tokenizer.decode(item[\"input_ids\"].tolist()[0], skip_special_tokens=False).replace(\"[NL]\", \"\\n\"))\n",
    "\n",
    "output_ids, raw_probs, probs = top_p_decode(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids=item[\"input_ids\"][0].unsqueeze(0),\n",
    "    start_tokens=tokenizer.encode(\"[SOSE]\").ids,\n",
    "    max_new_tokens=200,\n",
    "    top_p=0.2,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_time_custom = time.perf_counter()\n",
    "\n",
    "output_ids, raw_probs, probs = top_p_decode(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids=item[\"input_ids\"][0].unsqueeze(0),\n",
    "    start_tokens=tokenizer.encode(\"[SOSE]\").ids,\n",
    "    max_new_tokens=200,\n",
    "    top_p=0.2,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time_custom = time.perf_counter()\n",
    "duration_custom = end_time_custom - start_time_custom\n",
    "\n",
    "print()\n",
    "print(f\"Tokens generated: {len(output_ids[0])}\")\n",
    "print(f\"Execution time: {duration_custom:.4f} second\")\n",
    "\n",
    "generated_ids_creative = model.generate(\n",
    "    input_ids=item['input_ids'].to(\"cuda\"),\n",
    "    #attention_mask=item['attention_mask'].to(\"cuda\"),\n",
    "    decoder_input_ids=torch.tensor([2]).unsqueeze(0).to(\"cuda\"),\n",
    "    max_new_tokens=5,\n",
    "    temperature=1,\n",
    "    top_k=50,\n",
    "    top_p=0.2,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_time_generate = time.perf_counter()\n",
    "\n",
    "generated_ids_creative = model.generate(\n",
    "    input_ids=item['input_ids'].to(\"cuda\"),\n",
    "    #attention_mask=item['attention_mask'].to(\"cuda\"),\n",
    "    decoder_input_ids=torch.tensor(tokenizer.encode(\"[SOSE]\").ids).unsqueeze(0).to(\"cuda\"),\n",
    "    max_new_tokens=200,\n",
    "    temperature=1,\n",
    "    top_k=3,\n",
    "    top_p=0.25,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time_generate = time.perf_counter()\n",
    "duration_generate = end_time_generate - start_time_generate\n",
    "\n",
    "print(f\"Tokens generated: {len(generated_ids_creative[0])}\")\n",
    "print(f\"Execution time: {duration_generate:.4f} second\")\n",
    "\n",
    "print()\n",
    "print(\"Denoised sequence (top-p autoregresive search)\")\n",
    "out_list = generated_ids_creative.tolist()\n",
    "#print(out_list)\n",
    "decoded_text = tokenizer.decode(out_list[0], skip_special_tokens=False).replace(\"[NL]\", \"\\n\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1392467-da75-4b00-9290-896a18b67719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for prob, raw_prob, token in zip(probs, raw_probs, out_list):\n",
    "    print(prob, raw_prob, token, tokenizer.decode([token], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb94d40-46b3-437e-9210-2bc67d68e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison with a professional model of similar size\n",
    "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "translator = pipeline(\"translation\", model=model_name)\n",
    "translated_output = translator(text_b2)\n",
    "\n",
    "print(\"\\n--- Результат перекладу ---\")\n",
    "print(f\"Original (DE): {german_text_b2.strip()}\")\n",
    "print(f\"Translation (EN): {translated_output[0]['translation_text']}\")\n",
    "\n",
    "\n",
    "translated_output_c2 = translator(text_c2)\n",
    "print(\"\\n--- Перевірка на тексті C2 ---\")\n",
    "print(f\"Original (DE): {german_text_c2.strip()}\")\n",
    "print(f\"Translation (EN): {translated_output_c2[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38a6f8-9bfd-4f53-a4d5-f7582735b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f925d5a-5c97-4a9b-84f4-7722598bc70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23d50f-aebf-4ff2-90df-942f1fbdfd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
