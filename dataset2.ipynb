{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a11717-4aa0-4e90-b9bc-9c4e1712b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from datasets import load_dataset, Dataset, interleave_datasets\n",
    "from tokenizers import Tokenizer\n",
    "from itertools import islice\n",
    "from datasets import get_dataset_config_names\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import zlib\n",
    "from typing import List, Dict\n",
    "from google import genai\n",
    "import time\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53c8f6-fa02-4571-b629-787e75649e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f76f55-c08e-48b3-be27-fd3e5f9cee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer_clean.json\")\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "EN_TOKEN_ID = 2\n",
    "DE_TOKEN_ID = 3\n",
    "EOS_TOKEN_ID = 4\n",
    "MASK_TOKEN_ID = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00557b5-22bc-42ef-a656-aaf17a38abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to corresponding code in model.ipynd\n",
    "# Loading en and de versions of oscar\n",
    "en = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_en',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "de = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_de',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Labeling data by language\n",
    "en = en.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"en\"})\n",
    "de = de.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"de\"})\n",
    "\n",
    "# Shuffling data\n",
    "buffer_size = 10000 \n",
    "seed = 42\n",
    "en = en.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "de = de.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "\n",
    "# interleaving data into one dataset\n",
    "streaming_dataset = interleave_datasets(\n",
    "    [en, de],\n",
    "    probabilities=[0.5, 0.5],\n",
    "    stopping_strategy=\"first_exhausted\",\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# stop words and paterns\n",
    "GENERAL_BAD_PATTERNS = re.compile(\n",
    "    r'''\n",
    "    \\b(\n",
    "        casino|gambling|poker|betting|slots?|roulette|blackjack|baccarat|craps|freespins|bonus|jackpot|wager|no deposit|ohne einzahlung|kostenlos spielen|echtes geld|spielautomaten|spielhalle|spielbank|willkommensbonus|startguthaben|casinospiele|\n",
    "        porn|porno|escort|erotic|hookup|onlyfans|nudes?|camgirls?|sexkontakte|erotik|sexchat|live sex|stripchat|webcamsex|geschlechtsverkehr|selbstbefriedigung|masturbation|pornos|pornhub|xvideos|xnxx|vibrators?|dicks?|cums?|\n",
    "        fast cash|bad credits?|zinsfrei|geld leihen|kredit aufnehmen|ratenzahlung|schnellkredit|binary options?|payday loans?|payday advance|cash advance|short-term loans?|no credit check|guaranteed loan|Kurzzeitkredite?|Minikredite?|Sofortkredite?|Kredit ohne Schufa|schnelles Geld|Geld sofort|\n",
    "        tinder|badoo|parship|elitepartner|lovoo|flirt|verlieben|\n",
    "        test answers?|cheat sheet|homework help|buy answers?|buy exam|abitur lösung|prüfung antworten|examen lösung|\n",
    "        bitcoin|ethereum|blockchain|nft|ico|airdrop|pump and dump|binance|coinbase|kraken|crypto trading|krypto|kryptowährung|\n",
    "        privacy policy|terms of use|terms and conditions|all rights reserved|copyright|impressum|datenschutz|nutzungsbedingungen|alle rechte vorbehalten|cookie policy|agb|rechtliche hinweise|haftungsausschluss|\n",
    "        viagra|levitra|cialis|penis|enlargement|erection|erektionsstörung|potenzmittel|libido|sexualstörung|\n",
    "        weight loss|fat burning|diet pills|appetite suppressant|abnehmen|diätpillen|fettverbrennung|schnell abnehmen|\n",
    "        make money online|side hustle|get rich quick|passives einkommen|geld verdienen|heimarbeit|schnell reich werden|\n",
    "        click here|buy now|order now|free trial|limited offer|jetzt kaufen|hier klicken|jetzt abonnieren|kostenlos testen|nur heute\n",
    "    )\\b\n",
    "    |\n",
    "    -{3,}|={3,}|\\*{3,}|\n",
    "    (?:(?:\\w+\\s*,\\s*){10,}\\w+)\n",
    "    ''',\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "# stop header words\n",
    "BOILERPLATE_HEADER_PATTERNS = re.compile(\n",
    "    r'^(?:\\s*)'\n",
    "    r'(you are not logged in|you do not have permission|access this page|'\n",
    "    r'terms of use|privacy policy|cookies?|all rights reserved|'\n",
    "    r'sign in|log in|register|create an account|register|'\n",
    "    r'skip to content|main navigation|toggle navigation|'\n",
    "    r'select language|choose your region|'\n",
    "    r'cheap|discounts?|easy|billige|rabatte|einfach|'\n",
    "    r'sie sind nicht angemeldet|kein zugriff|'\n",
    "    r'zur hauptnavigation|navigation überspringen|'\n",
    "    r'anmelden|einloggen|registrieren|konto erstellen|'\n",
    "    r'nutzungsbedingungen|datenschutz|cookies?|'\n",
    "    r'alle rechte vorbehalten|sprache auswählen|region wählen|'\n",
    "    r'günstig|rabatt|einfach|schnell|kostenlos|angebot|aktionen|'\n",
    "    r'jetzt anmelden|mehr erfahren|hier klicken|'\n",
    "    r'help|hilfe|assist|unterstützen|call|anrufen|send|senden|respond|antworten|fill|ausfüllen)',\n",
    "\n",
    "    re.IGNORECASE | re.MULTILINE\n",
    ")\n",
    "\n",
    "# One of the many tested filters. No useful patterns were found.\n",
    "def entropy(text):\n",
    "    freqs = {}\n",
    "    for char in text:\n",
    "        freqs[char] = freqs.get(char, 0) + 1\n",
    "\n",
    "    total = sum(freqs.values())\n",
    "    probs = [count / total for count in freqs.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "\n",
    "def filter_texts(example: dict,\n",
    "                 min_num_of_words = 100,\n",
    "                 max_digit_ratio=0.18, \n",
    "                 min_alpha_word_ratio=0.75, \n",
    "                 max_symbol_ratio=0.1, \n",
    "                 header_check_length=200, \n",
    "                 allowed_uppercase_ratio=0.07, \n",
    "                 logging=False) -> bool:\n",
    "    text = example[\"text\"]\n",
    "    lang = example[\"lang\"]\n",
    "    \n",
    "    # Base len filter\n",
    "    if not text or len(text) < 384:\n",
    "        return False\n",
    "\n",
    "    # Stop patterns filter\n",
    "    if GENERAL_BAD_PATTERNS.search(text):\n",
    "        if logging:\n",
    "            print(\"general bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Header stop words filter\n",
    "    text_header = text[:header_check_length]\n",
    "    if BOILERPLATE_HEADER_PATTERNS.search(text_header):\n",
    "        if logging:\n",
    "            print(\"header bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Statistical filters:\n",
    "    num_digits = 0\n",
    "    num_alpha_words = 0\n",
    "    words = text.lower().split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Filter by number of total words\n",
    "    if num_words < min_num_of_words:\n",
    "        if logging:\n",
    "            print(\"num words\")\n",
    "        return False\n",
    "\n",
    "    # Filter ttr\n",
    "    cleaned_words = [word.strip(\".,!?;:`'\\\"\") for word in words]\n",
    "    ttr = len(cleaned_words) / num_words\n",
    "    if num_words < 500:\n",
    "        ttr_threshold = 0.4\n",
    "    elif num_words < 2000:\n",
    "        ttr_threshold = 0.35\n",
    "    else:\n",
    "        ttr_threshold = 0.3\n",
    "        \n",
    "    if ttr < ttr_threshold:\n",
    "        if logging:\n",
    "            print(\"unique words\")\n",
    "        return False\n",
    "        \n",
    "    # Filter by numerical digits and alpha words\n",
    "    for word in words:\n",
    "        num_digits += sum(c.isdigit() for c in word)\n",
    "        if word.replace(\"`\", \"\").replace(\"'\", \"\").isalpha():\n",
    "            num_alpha_words += 1\n",
    "\n",
    "    if (num_digits / len(text)) > max_digit_ratio:\n",
    "        if logging:\n",
    "            print(\"digit words\")\n",
    "        return False\n",
    "\n",
    "    if (num_alpha_words / num_words) < min_alpha_word_ratio:\n",
    "        if logging:\n",
    "            print(\"alpha ratio\")\n",
    "        return False\n",
    "\n",
    "    # Filter by mean word lean\n",
    "    mean_word_len = sum(len(w) for w in words) / num_words\n",
    "    if not (3 < mean_word_len < (15 if lang == 'de' else 12)):\n",
    "        if logging:\n",
    "            print(\"word len\")\n",
    "        return False\n",
    "\n",
    "    # Filter by uppercase ratio\n",
    "    uppercase_chars_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    if uppercase_chars_ratio > allowed_uppercase_ratio:\n",
    "        if logging:\n",
    "            print(\"uppercase ratio\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenizer_test(batch, max_len=320, min_len=256, max_avg_token_id=7000, min_avg_token_len=3.2, max_avg_token_len=5, max_unk_count=3):\n",
    "    # Tokenize and filter\n",
    "    texts = []\n",
    "    # Text standardization\n",
    "    replacements = {\n",
    "        \"\\n\": \"[NL]\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"„\": '\"',\n",
    "        \"’\": \"'\",\n",
    "        \"—\": \"-\",\n",
    "        \"…\": \"...\",\n",
    "        \"`\": \"'\",\n",
    "        \"''\": '\"',\n",
    "        \"$\": \"dollars\",\n",
    "        \"€\": \"euros\",\n",
    "        \"½\": \"1/2\"\n",
    "    }\n",
    "    for i in range(len(batch[\"text\"])):\n",
    "        text = batch[\"text\"][i]\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        texts.append(text)\n",
    "        \n",
    "    # Get encodings\n",
    "    encodings = tokenizer.encode_batch(texts, add_special_tokens=False)\n",
    "    batch_test = [] \n",
    "    batch_token_ids = [None]*len(batch[\"text\"])\n",
    "    \n",
    "    for i, enc in enumerate(encodings):\n",
    "        token_ids = enc.ids\n",
    "        max_tokens_per_text = max_len - 1\n",
    "        # Trim text by max_len and filter by min_len\n",
    "        if len(token_ids) > max_tokens_per_text:\n",
    "            token_ids = token_ids[:max_tokens_per_text]\n",
    "        if len(token_ids) < min_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "\n",
    "        # Filter by unc tokens\n",
    "        unk_count = sum(1 for t in token_ids if t == UNK_TOKEN_ID)\n",
    "        if unk_count >= max_unk_count:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token id\n",
    "        avg_token_id = np.mean(token_ids)\n",
    "        if avg_token_id > max_avg_token_id:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by strange tokens\n",
    "        tokens_as_strings = enc.tokens\n",
    "        if not tokens_as_strings:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token len\n",
    "        avg_token_len = sum(len(t) for t in tokens_as_strings) / len(tokens_as_strings)\n",
    "        if max_avg_token_len < avg_token_len < min_avg_token_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "        # Label text as correct and add batch id\n",
    "        batch_test.append(True)\n",
    "        batch_token_ids[i] = token_ids\n",
    "        \n",
    "    return {\n",
    "        \"tokenizer_test\": batch_test,\n",
    "        \"token_ids\": batch_token_ids\n",
    "    }\n",
    "\n",
    "def filter_by_tokenizer_test(example):\n",
    "    # Separated from tokenizer_test because it allows tokenizing texts in batches and returning token_ids.\n",
    "    return example[\"tokenizer_test\"]\n",
    "\n",
    "streaming_dataset = streaming_dataset.filter(filter_texts)\n",
    "streaming_dataset = streaming_dataset.map(tokenizer_test, batched=True, batch_size=BATCH_SIZE*8,)\n",
    "streaming_dataset = streaming_dataset.filter(filter_by_tokenizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957a570-49de-4aae-a203-c4d1a5ce6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_dataset = streaming_dataset.skip(85000)\n",
    "iterator = iter(streaming_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c5925-431b-4b3d-8bcf-b60acdb934d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iterator)\n",
    "item[\"text\"][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ac7a1-b156-4356-96d5-b3580daaf933",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7d71d-dfbb-4776-9c0a-71ef94dc59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_requests(n, iterator):\n",
    "    \"\"\"\n",
    "    Preparing list of requests for batch processing.\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    request_metadata = {}\n",
    "\n",
    "    for i in range(n):\n",
    "        succes = False\n",
    "        while not succes:\n",
    "            sent_len = random.randint(1, 7)\n",
    "            skip = random.randint(1, 5)\n",
    "            item = next(iterator)\n",
    "            translate_to = [\"en\", \"de\"]\n",
    "            translate_from = item[\"lang\"]\n",
    "            translate_to.remove(translate_from)\n",
    "            translate_to = translate_to[0]\n",
    "\n",
    "            if item[\"lang\"] == \"en\":\n",
    "                sentences = nltk.sent_tokenize(item[\"text\"], language='english')\n",
    "            else:\n",
    "                sentences = nltk.sent_tokenize(item[\"text\"], language='german')\n",
    "            \n",
    "            text = \"\"\n",
    "            try:\n",
    "                sentences = sentences[skip:]\n",
    "                for sentence in sentences[:sent_len]:\n",
    "                    text += sentence + \" \"\n",
    "                text = text.strip()\n",
    "                if len(text) < sent_len * 108:\n",
    "                    succes = True\n",
    "            except IndexError:\n",
    "                succes = False\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a fluent English and German translator.\n",
    "Your task is to complete the following form:\n",
    "Original [{translate_from}] text (may break off):\n",
    "{text}\n",
    "Translated to [{translate_to}] version:\n",
    "Do not explain or comment. Do not add headings or labels. Only output the translated text:\n",
    "        \"\"\"\n",
    "        \n",
    "        request_key = f\"request_{i}\"\n",
    "        \n",
    "        batch_requests.append({\n",
    "            \"key\": request_key,\n",
    "            \"request\": {\n",
    "                \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        request_metadata[request_key] = {\n",
    "            \"original_text\": text,\n",
    "            \"lang_from\": translate_from,\n",
    "            \"lang_to\": translate_to\n",
    "        }\n",
    "        \n",
    "    return batch_requests, request_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298527b-78cf-4d11-ab9b-be51f23726fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_batch_results(result_file_content, metadata):\n",
    "    \"\"\"\n",
    "    Process results from file, getted from batch work\n",
    "    \"\"\"\n",
    "    pairs = {\"en\": [], \"de\": []}\n",
    "    \n",
    "    result_lines = result_file_content.decode('utf-8').strip().split('\\n')\n",
    "    \n",
    "    for line in result_lines:\n",
    "        if not line: continue\n",
    "        result = json.loads(line)\n",
    "        request_key = result[\"key\"]\n",
    "        \n",
    "        meta = metadata.get(request_key)\n",
    "        if not meta:\n",
    "            print(f\"Warning: No metadata found for key {request_key}\")\n",
    "            continue\n",
    "\n",
    "        if \"response\" in result:\n",
    "            try:\n",
    "                translated_text = result[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "                pairs[meta[\"lang_from\"]].append(meta[\"original_text\"])\n",
    "                pairs[meta[\"lang_to\"]].append(translated_text.strip())\n",
    "            except (KeyError, IndexError):\n",
    "                print(f\"Request with key {request_key} had an unexpected response format: {result['response']}\")\n",
    "        elif \"status\" in result:\n",
    "            error_message = result[\"status\"].get(\"message\", \"No message provided\")\n",
    "            error_code = result[\"status\"].get(\"code\", \"N/A\")\n",
    "            print(f\"Request with key {request_key} failed. Code: {error_code}, Message: {error_message}\")\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f30fd-1e69-48b9-894f-6603203a4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_data_batch(n, iterator):\n",
    "    \"\"\"\n",
    "    Sending batch to gemini model\n",
    "    \"\"\"\n",
    "    client = genai.Client()\n",
    "\n",
    "    print(f\"1. Preparing {n} requests...\")\n",
    "    requests_to_process, metadata = prepare_batch_requests(n, iterator)\n",
    "\n",
    "    input_filename = \"batch_requests.jsonl\"\n",
    "    with open(input_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for req in requests_to_process:\n",
    "            f.write(json.dumps(req) + \"\\n\")\n",
    "\n",
    "    print(f\"2. Uploading the input file: {input_filename}\")\n",
    "    uploaded_file = client.files.upload(\n",
    "        file=input_filename,\n",
    "        config={'display_name': 'Batch Translation Requests', 'mime_type': 'jsonl'}\n",
    "    )\n",
    "\n",
    "    print(\"3. Creating the batch job for model...\")\n",
    "    batch_job = client.batches.create(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        src=uploaded_file.name,\n",
    "        config={'display_name': \"translation-job\"}\n",
    "    )\n",
    "\n",
    "    print(f\"   Job created with name: {batch_job.name}\")\n",
    "    return batch_job.name, uploaded_file.name, metadata\n",
    "\n",
    "\n",
    "def collect_data_batch(batch_job_name, uploaded_file_name, metadata, cleanup=False):\n",
    "    client = genai.Client()\n",
    "\n",
    "    print(\"4. Waiting for the batch job to complete...\")\n",
    "    while True:\n",
    "        batch_job = client.batches.get(name=batch_job_name)\n",
    "        if batch_job.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED'):\n",
    "            break\n",
    "        print(f\"   Current job state: {batch_job.state.name}. Polling again in 30 seconds...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    print(f\"   Job finished with state: {batch_job.state.name}\")\n",
    "\n",
    "    if batch_job.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        result_file_name = batch_job.dest.file_name\n",
    "        print(f\"5. Downloading results from: {result_file_name}\")\n",
    "        result_content = client.files.download(file=result_file_name)\n",
    "\n",
    "        print(\"6. Processing results...\")\n",
    "        final_pairs = process_batch_results(result_content, metadata)\n",
    "\n",
    "        if cleanup:\n",
    "            print(\"7. Cleaning up uploaded files...\")\n",
    "            client.files.delete(name=uploaded_file_name)\n",
    "            client.files.delete(name=result_file_name)\n",
    "\n",
    "        return final_pairs\n",
    "    else:\n",
    "        print(f\"Batch job failed. Error: {batch_job.error}\")\n",
    "        if cleanup:\n",
    "            client.files.delete(name=uploaded_file_name.replace(\"files/\", \"\"))\n",
    "        return {\"en\": [], \"de\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fc85f-0ab4-4d22-9a07-c6e3d3941c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_name_new, uploaded_file_name_new, metadata_new = send_data_batch(14000, iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d08dc-e101-4b5e-8617-e54b81cf461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_config = {\n",
    "    \"batch_job_name\": batch_job_name_new,\n",
    "    \"uploaded_file_name\": uploaded_file_name_new,\n",
    "    \"metadata\": metadata_new\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f1380-50ed-4c91-9fc6-b89e3a2e562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"request_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(request_config, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd721d-3d14-4674-8f27-5c3436bb6494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15a1be-27a8-48ef-bba9-e6f8370bc5fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"request_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    request_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11500b-e608-47dd-baf6-afe901d2f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_name, uploaded_file_name, metadata = request_config[\"batch_job_name\"], request_config[\"uploaded_file_name\"], request_config[\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc486d61-b1fc-483e-8da1-6d7f9c021b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = collect_data_batch(batch_job_name, uploaded_file_name, metadata)\n",
    "with open(\"translations6.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60020552-539c-4bf3-94ef-03f298bab586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(zip(results[\"en\"], results[\"de\"])):\n",
    "    if i<5:\n",
    "        print(\"===============================================================\")\n",
    "        print(pair[0])\n",
    "        print()\n",
    "        print(pair[1])\n",
    "        print(\"===============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a636f9d-ec32-4cd9-a3c5-6ee3a9116467",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"translations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part1 = json.load(f)\n",
    "with open(\"translations2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part2 = json.load(f)\n",
    "with open(\"translations3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part3 = json.load(f)\n",
    "with open(\"translations4.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part4 = json.load(f)\n",
    "with open(\"translations5.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part5 = json.load(f)\n",
    "with open(\"translations6.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_part6 = json.load(f)\n",
    "    \n",
    "text_pairs = {\n",
    "    \"en\": data_part1[\"en\"] + data_part2[\"en\"] + data_part3[\"en\"] + data_part4[\"en\"] + data_part5[\"en\"] + data_part6[\"en\"],\n",
    "    \"de\": data_part1[\"de\"] + data_part2[\"de\"] + data_part3[\"de\"] + data_part4[\"de\"] + data_part5[\"de\"] + data_part6[\"de\"]\n",
    "}\n",
    "\n",
    "with open(\"oscar-en-de-synthetic.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(text_pairs, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50e0ee-a370-43b0-bdf4-4880698e5952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
