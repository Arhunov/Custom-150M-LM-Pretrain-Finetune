{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a075d-9113-4145-85ca-9e63d29c7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace \n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import Metaspace as MetaspaceDecoder\n",
    "from tokenizers import normalizers\n",
    "import itertools\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import Dataset, load_dataset, interleave_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bcae8-595e-4dcc-857f-35d4d1550267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to comprehensive block in model.ipynb\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "EN_TOKEN_ID = 2\n",
    "DE_TOKEN_ID = 3\n",
    "EOS_TOKEN_ID = 4\n",
    "MASK_TOKEN_ID = 5\n",
    "\n",
    "clean_texts = {\"tokenizer_approved\": []}\n",
    "\n",
    "# Loading en and de versions of oscar\n",
    "en = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_en',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "de = load_dataset(\n",
    "    'oscar',\n",
    "    name='unshuffled_deduplicated_de',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Labeling data by language\n",
    "en = en.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"en\"})\n",
    "de = de.map(lambda ex: {\"text\": ex[\"text\"], \"lang\": \"de\"})\n",
    "\n",
    "# Shuffling data\n",
    "buffer_size = 10000 \n",
    "seed = 42\n",
    "en = en.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "de = de.shuffle(seed=seed, buffer_size=buffer_size)\n",
    "\n",
    "# interleaving data into one dataset\n",
    "streaming_dataset = interleave_datasets(\n",
    "    [en, de],\n",
    "    probabilities=[0.5, 0.5],\n",
    "    stopping_strategy=\"first_exhausted\",\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# stop words and paterns\n",
    "GENERAL_BAD_PATTERNS = re.compile(\n",
    "    r'''\n",
    "    \\b(\n",
    "        casino|gambling|poker|betting|slots?|roulette|blackjack|baccarat|craps|freespins|bonus|jackpot|wager|no deposit|ohne einzahlung|kostenlos spielen|echtes geld|spielautomaten|spielhalle|spielbank|willkommensbonus|startguthaben|casinospiele|\n",
    "        porn|porno|escort|erotic|hookup|onlyfans|nudes?|camgirls?|sexkontakte|erotik|sexchat|live sex|stripchat|webcamsex|geschlechtsverkehr|selbstbefriedigung|masturbation|pornos|pornhub|xvideos|xnxx|vibrators?|dicks?|cums?|\n",
    "        fast cash|bad credits?|zinsfrei|geld leihen|kredit aufnehmen|ratenzahlung|schnellkredit|binary options?|payday loans?|payday advance|cash advance|short-term loans?|no credit check|guaranteed loan|Kurzzeitkredite?|Minikredite?|Sofortkredite?|Kredit ohne Schufa|schnelles Geld|Geld sofort|\n",
    "        tinder|badoo|parship|elitepartner|lovoo|flirt|verlieben|\n",
    "        test answers?|cheat sheet|homework help|buy answers?|buy exam|abitur lösung|prüfung antworten|examen lösung|\n",
    "        bitcoin|ethereum|blockchain|nft|ico|airdrop|pump and dump|binance|coinbase|kraken|crypto trading|krypto|kryptowährung|\n",
    "        privacy policy|terms of use|terms and conditions|all rights reserved|copyright|impressum|datenschutz|nutzungsbedingungen|alle rechte vorbehalten|cookie policy|agb|rechtliche hinweise|haftungsausschluss|\n",
    "        viagra|levitra|cialis|penis|enlargement|erection|erektionsstörung|potenzmittel|libido|sexualstörung|\n",
    "        weight loss|fat burning|diet pills|appetite suppressant|abnehmen|diätpillen|fettverbrennung|schnell abnehmen|\n",
    "        make money online|side hustle|get rich quick|passives einkommen|geld verdienen|heimarbeit|schnell reich werden|\n",
    "        click here|buy now|order now|free trial|limited offer|jetzt kaufen|hier klicken|jetzt abonnieren|kostenlos testen|nur heute\n",
    "    )\\b\n",
    "    |\n",
    "    -{3,}|={3,}|\\*{3,}|\n",
    "    (?:(?:\\w+\\s*,\\s*){10,}\\w+)\n",
    "    ''',\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "# stop header words\n",
    "BOILERPLATE_HEADER_PATTERNS = re.compile(\n",
    "    r'^(?:\\s*)'\n",
    "    r'(you are not logged in|you do not have permission|access this page|'\n",
    "    r'terms of use|privacy policy|cookies?|all rights reserved|'\n",
    "    r'sign in|log in|register|create an account|register|'\n",
    "    r'skip to content|main navigation|toggle navigation|'\n",
    "    r'select language|choose your region|'\n",
    "    r'cheap|discounts?|easy|billige|rabatte|einfach|'\n",
    "    r'sie sind nicht angemeldet|kein zugriff|'\n",
    "    r'zur hauptnavigation|navigation überspringen|'\n",
    "    r'anmelden|einloggen|registrieren|konto erstellen|'\n",
    "    r'nutzungsbedingungen|datenschutz|cookies?|'\n",
    "    r'alle rechte vorbehalten|sprache auswählen|region wählen|'\n",
    "    r'günstig|rabatt|einfach|schnell|kostenlos|angebot|aktionen|'\n",
    "    r'jetzt anmelden|mehr erfahren|hier klicken|'\n",
    "    r'help|hilfe|assist|unterstützen|call|anrufen|send|senden|respond|antworten|fill|ausfüllen)',\n",
    "\n",
    "    re.IGNORECASE | re.MULTILINE\n",
    ")\n",
    "\n",
    "# One of the many tested filters. No useful patterns were found.\n",
    "def entropy(text):\n",
    "    freqs = {}\n",
    "    for char in text:\n",
    "        freqs[char] = freqs.get(char, 0) + 1\n",
    "\n",
    "    total = sum(freqs.values())\n",
    "    probs = [count / total for count in freqs.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "\n",
    "def filter_texts(example: dict,\n",
    "                 min_num_of_words = 100,\n",
    "                 max_digit_ratio=0.18, \n",
    "                 min_alpha_word_ratio=0.75, \n",
    "                 max_symbol_ratio=0.1, \n",
    "                 header_check_length=200, \n",
    "                 allowed_uppercase_ratio=0.07, \n",
    "                 logging=False) -> bool:\n",
    "    text = example[\"text\"]\n",
    "    lang = example[\"lang\"]\n",
    "    \n",
    "    # Base len filter\n",
    "    if not text or len(text) < 384:\n",
    "        return False\n",
    "\n",
    "    # Stop patterns filter\n",
    "    if GENERAL_BAD_PATTERNS.search(text):\n",
    "        if logging:\n",
    "            print(\"general bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Header stop words filter\n",
    "    text_header = text[:header_check_length]\n",
    "    if BOILERPLATE_HEADER_PATTERNS.search(text_header):\n",
    "        if logging:\n",
    "            print(\"header bad pattern\")\n",
    "        return False\n",
    "\n",
    "    # Statistical filters:\n",
    "    num_digits = 0\n",
    "    num_alpha_words = 0\n",
    "    words = text.lower().split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Filter by number of total words\n",
    "    if num_words < min_num_of_words:\n",
    "        if logging:\n",
    "            print(\"num words\")\n",
    "        return False\n",
    "\n",
    "    # Filter ttr\n",
    "    cleaned_words = [word.strip(\".,!?;:`'\\\"\") for word in words]\n",
    "    ttr = len(cleaned_words) / num_words\n",
    "    if num_words < 500:\n",
    "        ttr_threshold = 0.4\n",
    "    elif num_words < 2000:\n",
    "        ttr_threshold = 0.35\n",
    "    else:\n",
    "        ttr_threshold = 0.3\n",
    "        \n",
    "    if ttr < ttr_threshold:\n",
    "        if logging:\n",
    "            print(\"unique words\")\n",
    "        return False\n",
    "        \n",
    "    # Filter by numerical digits and alpha words\n",
    "    for word in words:\n",
    "        num_digits += sum(c.isdigit() for c in word)\n",
    "        if word.replace(\"`\", \"\").replace(\"'\", \"\").isalpha():\n",
    "            num_alpha_words += 1\n",
    "\n",
    "    if (num_digits / len(text)) > max_digit_ratio:\n",
    "        if logging:\n",
    "            print(\"digit words\")\n",
    "        return False\n",
    "\n",
    "    if (num_alpha_words / num_words) < min_alpha_word_ratio:\n",
    "        if logging:\n",
    "            print(\"alpha ratio\")\n",
    "        return False\n",
    "\n",
    "    # Filter by mean word lean\n",
    "    mean_word_len = sum(len(w) for w in words) / num_words\n",
    "    if not (3 < mean_word_len < (15 if lang == 'de' else 12)):\n",
    "        if logging:\n",
    "            print(\"word len\")\n",
    "        return False\n",
    "\n",
    "    # Filter by uppercase ratio\n",
    "    uppercase_chars_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    if uppercase_chars_ratio > allowed_uppercase_ratio:\n",
    "        if logging:\n",
    "            print(\"uppercase ratio\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenizer_test(batch, max_len=320, min_len=256, max_avg_token_id=7000, min_avg_token_len=3.2, max_avg_token_len=5, max_unk_count=3):\n",
    "    # Tokenize and filter\n",
    "    texts = []\n",
    "    # Text standardization\n",
    "    replacements = {\n",
    "        \"\\n\": \"[NL]\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"„\": '\"',\n",
    "        \"’\": \"'\",\n",
    "        \"—\": \"-\",\n",
    "        \"…\": \"...\",\n",
    "        \"`\": \"'\",\n",
    "        \"''\": '\"',\n",
    "        \"$\": \"dollars\",\n",
    "        \"€\": \"euros\",\n",
    "        \"½\": \"1/2\"\n",
    "    }\n",
    "    for i in range(len(batch[\"text\"])):\n",
    "        text = batch[\"text\"][i]\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        texts.append(text)\n",
    "        \n",
    "    # Get encodings\n",
    "    encodings = tokenizer.encode_batch(texts, add_special_tokens=False)\n",
    "    batch_test = [] \n",
    "    batch_token_ids = [None]*len(batch[\"text\"])\n",
    "    \n",
    "    for i, enc in enumerate(encodings):\n",
    "        token_ids = enc.ids\n",
    "        max_tokens_per_text = max_len - 1\n",
    "        # Trim text by max_len and filter by min_len\n",
    "        if len(token_ids) > max_tokens_per_text:\n",
    "            token_ids = token_ids[:max_tokens_per_text]\n",
    "        if len(token_ids) < min_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "\n",
    "        # Filter by unc tokens\n",
    "        unk_count = sum(1 for t in token_ids if t == UNK_TOKEN_ID)\n",
    "        if unk_count >= max_unk_count:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token id\n",
    "        avg_token_id = np.mean(token_ids)\n",
    "        if avg_token_id > max_avg_token_id:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by strange tokens\n",
    "        tokens_as_strings = enc.tokens\n",
    "        if not tokens_as_strings:\n",
    "            batch_test.append(False)\n",
    "            continue \n",
    "\n",
    "        # Filter by average token len\n",
    "        avg_token_len = sum(len(t) for t in tokens_as_strings) / len(tokens_as_strings)\n",
    "        if max_avg_token_len < avg_token_len < min_avg_token_len:\n",
    "            batch_test.append(False)\n",
    "            continue\n",
    "        # Label text as correct and add batch id\n",
    "        batch_test.append(True)\n",
    "        batch_token_ids[i] = token_ids\n",
    "        clean_texts[\"tokenizer_approved\"].append(batch[\"text\"][i])\n",
    "        \n",
    "    return {\n",
    "        \"tokenizer_test\": batch_test,\n",
    "        \"token_ids\": batch_token_ids\n",
    "    }\n",
    "\n",
    "def filter_by_tokenizer_test(example):\n",
    "    return example[\"tokenizer_test\"]\n",
    "\n",
    "streaming_dataset = streaming_dataset.filter(filter_texts)\n",
    "streaming_dataset = streaming_dataset.map(tokenizer_test, batched=True, batch_size=128,)\n",
    "streaming_dataset = streaming_dataset.filter(filter_by_tokenizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fba5f3-bde6-4ab0-801f-ee282c15f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(streaming_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8533fd-2115-4101-bd65-3e9acdf52afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"{i}) steps\")\n",
    "    next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3dee7-6879-4b2d-9ef3-dfd48e81284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_texts[\"tokenizer_approved\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526f5ed-27c7-4658-9dc4-16b4e43bd87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Used for creating first tokenizer\n",
    "\"\"\"\n",
    "text_tokenizer_en = \"\"\n",
    "i = 0\n",
    "for example in itertools.islice(dataset_en_stream, 8000):\n",
    "    i+=1\n",
    "    text_tokenizer_en += example['text']\n",
    "    time.sleep(0.008)\n",
    "    if i%100==0:\n",
    "        print(f\"added {i} en texts\")\n",
    "        \n",
    "i = 0\n",
    "text_tokenizer_de = \"\"\n",
    "for example in itertools.islice(dataset_de_stream, 12000):\n",
    "    i+=1\n",
    "    text_tokenizer_de += example['text']\n",
    "    time.sleep(0.008)\n",
    "    if i%100==0:\n",
    "        print(f\"added {i} de texts\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6e122-c3b5-4ad2-a5e3-29deeafc0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_tokenizer_de[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61d8c2-7409-4cbe-ad86-ff2aa5260209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_tokenizer(text: str) -> str:\n",
    "    allowed_chars = r\"a-zA-ZäöüßÄÖÜ0-9\\s.,'!?\\\"\\-():;&\"\n",
    "    cleaned_text = re.sub(f'[^{allowed_chars}]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52322e01-b20e-40d4-b232-967e513f1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_texts[\"tokenizer_approved\"])):\n",
    "    clean_texts[\"tokenizer_approved\"][i] = clean_text_for_tokenizer(clean_texts[\"tokenizer_approved\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51577606-13c0-48f7-bc89-38e0e42c78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_tokenizer_de = text_tokenizer_de.replace('\\n', ' [NL]')\n",
    "#text_tokenizer_en = text_tokenizer_en.replace('\\n', ' [NL]')\n",
    "\n",
    "#text_tokenizer_de = clean_text_for_tokenizer(text_tokenizer_de)\n",
    "#text_tokenizer_en = clean_text_for_tokenizer(text_tokenizer_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05c1a3-4a12-4fe6-a7a8-659040456366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = (text_tokenizer_en+text_tokenizer_de).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf44b10-6ab8-45d1-ae8d-b4aabcf44cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83649a27-ac82-45c4-a795-fb45d8459f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = Metaspace()\n",
    "tokenizer.decoder = MetaspaceDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bc1bc-bd94-43aa-8741-11daa2031903",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UnigramTrainer(\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOSE]\", \"[SOSD]\", \"[EOS]\", \"[MASK]\", \"[NL]\"], # sose - start of the sequence english, sosd - start of the sequence deutch\n",
    "    unk_token=\"[UNK]\",\n",
    "    max_piece_length=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c977b2-c83f-4196-b217-24c402f993e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(clean_texts[\"tokenizer_approved\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be681d9-e5ec-4e66-8def-efa4622fa73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Übersetzung ist ein entscheidender Bestandteil des maschinellen lernens.\").tokens)\n",
    "print(tokenizer.encode(\"Translation is very important in machine learning.\").tokens)\n",
    "print(tokenizer.encode(\"Das hierarchische Bayes-Modell wurde modifiziert.\").tokens)\n",
    "print(tokenizer.encode(\"This is uncharacteristically difficult.\").tokens)\n",
    "print(tokenizer.encode(\"Let's test antidisestablishmentarianism.\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca68c4-e06b-4bd8-b53b-6e5e64adce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer_clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966722ce-d3ad-411d-bfd1-580299fbf8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805da94-0580-4979-ad3f-cae3394b4d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
